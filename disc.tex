%!TEX root =  lsa.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:discussion}
General discussion + step-size dependence on problem instances

\paragraph{Bias and Variance:} The means-squared error at $t$ is bounded by a sum of two terms. The first term is the bias term given by $\B=(1+4(\alpha\rho_{\alpha})^{-1}) (\alpha\rho_{\alpha})^{-1} \Big(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}\Big)$.  The \emph{Bias} term that captures the rate at which the initial condition $\norm{\theta_t-\ts}^2$ is forgotten. The second term is the \emph{Variance} given by $\V=(1+4(\alpha\rho_{\alpha})^{-1}) (\alpha\rho_{\alpha})^{-1} \Big(\frac{\alpha^2\sigma^2}{t+1} \Big)$. The variance term that captures the rate at which noise is rejected. The $\B$ and $\V$ terms capture two different sources of errors. To see this, note that in the absence of noise i.e., when $\zeta_t=0,~\forall t\geq 0$, we have $\E[\norm{H\eb_t}^2]=\B$ and in the presence of noise, i.e., $\zeta\neq 0,\forall t\geq 0$ and the perfect initial condition, i.e., $\theta_t=\ts$, we have $\E[\norm{H\eb_t}^2]=\V$.

\paragraph{Faster Rates:} The bias term term decays $O(1/t^2)$ and the variance term decays $O(1/t)$, which are faster than the rates that can be achieved by a diminishing stepsize sequence recommended by standard stochastic approximation theory \cite{SA}. In particular, when the stepsizes are decaying $O(1/t)$ one can only achieve an rate that is $O(1/t^\mu)$ where $\mu$ is the smallest real part of the eigenvalue of $H$. Thus the choice of a constant stepsize sequence with iterate averaging on top helps us to eliminate the effect of `ill' conditioning of $H$.

\paragraph{Problem Dependent terms:} The choice of the stepsize is unfortunately problem dependent. To see this, the condition that $\rho_{\alpha}>0$ in \Cref{maintheorem} ensures that the expected spectral norm of $\E[(I-\alpha H_t)^\top(I-\alpha H_t)]$ is less than unity. And thus the range of $\alpha$ changes from problem to problem, an issue we further elaborate in \Cref{sec:stepprob}.

\paragraph{Behaviour for extreme value of $\alpha$:} For smaller values of stepsize, i.e., $\alpha\approx 0$, the bias term blows up, due to the presence of $\alpha^{-1}$ term. This is due to the fact that the stepsizes determine the learning rate and for smaller stepsizes the learning rate is slower. However, in this case the noise term does not blow up, a fact that can appreciated by looking at \eqref{lsergen} where $\alpha$ is seen to multiply the noise term $\zeta_t$. In quantitative terms, we can see that the $\alpha^{-2}$ and $\alpha^2$ terms can each other. For larger values of $\alpha$ i.e., $\alpha\ra \alpha_{\max}$, the bounds blow up again, due to the fact that $\rho_{\alpha}\ra 0$ in this case. This is due to the fact that the effect of both noise and initial conditions decay with the contraction factor of $F_{t,i}$, which gets closer to unity as $\alpha\ra\alpha_{\max}$.


\subsection{Step Size dependence on Problem Instance}
\label{sec:stepprob}
We discussed in \Cref{sec:prob} that the maximum stepsize depends on the problem instance. The idea is to choose the stepsize that ensures stability of the recursion, which in turn translates to contraction properties of the product of random matrices. We now elaborate on two notions of contractions namely 
\emph{(i)} contraction per step, \emph{(ii)} contraction of the product.

\paragraph{Contraction Per Time Step}
For a distribution $P$ over square $n\times n$ real matrices let $H_P = \int M dP(M)$, $Q_P = \int M^\top M dP(M)$ and
\begin{align*}
\rho_\alpha(P) = \inf_{\norm{x}=1} x^\top (H_P^\top + H_P - \alpha Q) x\,.
\end{align*}
\newcommand{\cP}{\mathcal{P}}
Call a set of distributions $\cP$ over square real matrices 
\emph{weakly admissible} if there exists $\alpha>0$ such that 
$\rho_\alpha(P)>0$ holds for all $P\in \cP$,
while it is called \emph{admissible} 
if there exists some $\alpha>0$ such that $\inf_{P\in \cP} \rho_\alpha(P)>0$. 
The value of $\alpha$ is called a witness. 
It is easy to see that $\alpha \mapsto \rho_\alpha(P)$ is decreasing, 
hence if $\alpha>0$ witnesses that $\cP$ is (weakly) admissible 
then any $0<\alpha'\le \alpha$ is also witnessing this.

If $\cP$ is weakly admissible, then one can choose a stepsize $\alpha>0$ solely based on the knowledge of $\cP$ and 
conclude that no matter the joint distribution of $(H_t,g_t)$, as long as the distribution of $H_t$ is an element of $\cP$, the conclusions of \cref{maintheorem} hold. Further, if $\cP$ is admissible then the error bound stated in  \cref{maintheorem} becomes independent of the instance.
Hence, an interesting question to investigate is whether a given set $\cP$ is (weakly) admissible.
For $B>0$, let $\cP_B$ is the set of distributions where for every $P\in \cP_B$, 
$H_P$ is nonsingular and if $H\sim P$ then $\norm{H}\le B$ holds with probability one.
In what follows all distributions will be a subset of $\cP_B$ with some $B>0$, 
Note that in most applications the knowledge of a suitable $B>0$ such that $\cP\subset \cP_B$ is available.
By appropriately normalizing the matrices involved, we may as well assume that $B=1$.

Our first result shows that we cannot be too ambitious:
\begin{proposition}
The set $\cP_1$ is not weakly admissible.
\end{proposition}
\begin{proof}
Fix an arbitrary $\alpha>0$. We show that there exists $P\in \cP$ such that $\rho_\alpha(P)<0$.
For $\epsilon \in (0,1/2)$ let $P$ be the distribution that is supported on $\{-I,I\}$ and takes on the value of $I$ with probability $1/2+\epsilon$. Then $H_P = 2\epsilon I \succ 0$, hence $P\in \cP_1$. Further, $Q_P = I$.
Hence, $\rho_\alpha(P) = 4\epsilon-\alpha$. Hence, if $\epsilon<\alpha/4$, $\rho_\alpha(P)<0$.
\end{proof}

In linear least-squares problems (cf. \cref{ex:leastsquares}) the set $\cP$ has more structure.
In particular, the random matrices $\{H_t\}$ are PSD. \todoc{The standard notation is $\mathrm{S}^{++}$ for the set of positive definite matrices. Introduce these and use them below..}
Define
\begin{align*}
\cP_{\text{PSD},B} = \{ P \in \cP_B \,:\,  \supp(P)\subset \text{PSD} \}\,.
\end{align*}
We have the following proposition:
\begin{proposition}
For any $B>0$, the set $\cP_{\text{PSD},B}$ is weakly admissible
and in particular any $0<\alpha < 2/B$ witnesses this.
\end{proposition}
\begin{proof}
Take any $P\in \cP_{\text{PSD}}$ and let $H\sim P$.
Consider the SVD of $H$: $H = U \Lambda U^\top$ where $U$ is orthonormal and $\Lambda$ is diagonal with
nonnegative elements. Note that $\Lambda \preceq B\, \eye$ and thus $\Lambda^2 \preceq B \Lambda$.
Then for any $x\in \R^n$, $x^\top H^\top H x = x^\top U \Lambda^2 U^\top x \le B x^\top U \Lambda U^\top x = B x^\top H x$.
Taking expectations we find that $x^\top Q_P x \le B x^\top H_P x$.
Hence, $\rho_\alpha(P) = 2 x^\top H_P x - \alpha x^\top Q_P x \ge (2- \alpha B ) \,x^\top H_P x $.
Thus, for any $\alpha<2/B$, $\rho_\alpha(P)>0$.
\end{proof}
\todoc[inline]{So this altogether is weaker than what Bach founds. They prove strong admissibility.
I am guessing that for this they use that the loss is special and also that the noise is special.
This would be a good place to somehow incorporate this with a (short?) proof.
}

\paragraph{Contraction over time $t$}
The condition of contraction per time step is overly restrictive in the general case (other than the one presented in \Cref{rankonestep}). We can relax this condition and ask whether a weaker result in \Cref{multheorem} holds.
\begin{lemma}\label{multheorem}
Let $\H$ be the space of $n\times n$ real matrices $H=(H_{ij}),i,j=1,\ldots,n$ with the absolute values of their entries bounded, i.e., $|H_{ij}|\leq B$ for some constant $B>0$. Let $\P$ be the space of distributions supported on $\M$ such that $~\forall x\in R^n, P\in \P,\E_{P}[x^\top H x] >0$ (where $H\in\H$ ). Then there exists an $\alpha_{\max}>0$ such that $\alpha\in(0,\alpha_{\max})$ implies $\us{\sup}{\norm{x}\leq 1,P\in\P,t\geq 0}\E_{P}[\norm{(I-\alpha H)^tx}]< \infty$.
\end{lemma}
\textbf{Possible Counter example for \Cref{multheorem} in $\R$}
Consider $\H=[-1,1]$ and $\P$ supported on $\{-1,1\}$ with $P(1)=\frac{1}{2}+\epsilon$ and $P(-1)=\frac{1}{2}-\epsilon$. Note that
\begin{align*}
\E[\Pi_{i=1}^t (1-\alpha H_i)]&=\exp{\ous{\sum}{i=1}{t}\log(1-\alpha H_i)}\\
&\approx \exp{t\E[\log(1-\alpha H_i)]}
\end{align*}
Now the condition $\E[\Pi_{i=1}^t (1-\alpha H_i)]<\infty,$ where $H_i\sim P$ loosely translates to the following
\begin{align*}
\E[\log(1-\alpha H_i)]<0,
\end{align*}
which can be verified by noting that
\begin{align*}
&\E[\log(1-\alpha H_i)]\\&=\E[(\frac{1}{2}+\epsilon)\log (1-\alpha)+(\frac{1}{2}-\epsilon)\log (1+\alpha)]\\
&=\frac{1}{2}\log(1-\alpha^2)+\epsilon(\log(1-\alpha)-\log(1+\alpha))\\
&<0
\end{align*}
\textbf{Possible Counter example for \Cref{multheorem} in $\R^2$}
Given any $\alpha$ the aim is to choose a distribution $P$ over $\H$ such that $\E_{P}[\norm{(I-\alpha H)^tx}]$ grows with $t$. In other words, given an $\alpha>0$, we are looking for a distribution $P'$ supported in the $\alpha$ neighbourhood of $I$ denoted by $\F$ such that $\E_{P'}[F^t]$ grows with $t$ (where $F\sim P'$). We now look at a possible suggestion for such a $P'$.\par
\begin{example}
Pick $F_1=\begin{bmatrix} 0 & 1-\beta_2 \\ 1+\beta_1& 0\\\end{bmatrix}^{\frac{1}{p}}$, $F_2=\begin{bmatrix} 0 & 1+\beta_1 \\ 1-\beta_2& 0\\\end{bmatrix}^{\frac{1}{p}}$, where $\beta_2>\beta_1$ such that $1+\frac{1}{2}(\beta_1-\beta_2)$. Let $P'$ be supported on $\{F_1,F_2\}$ with $P'(F_1)=\frac{1}{2}$ and $P'(F_2)=\frac{1}{2}$, and  $p$ is sufficiently large such that $F_1,F_2$ belong to the $\alpha$-neighbourhood of $I$. A possible way the random product can blow up is when products $F_2^pF_1^p$ occur in a contiguous stream. To see this,
\begin{align}
F_2^pF_1^p=\begin{bmatrix} (1+\beta_1) &0  \\ 0& (1-\beta_2)^2\\\end{bmatrix}
\end{align}
Thus $\norm{F_2^p F_1^p}>1$. However, such (i.e., $F_2^p F_1^p$) sequences have probability of occurance $\frac{1}{2^{2p}}$ is a sequence of length $2p$. Thus probability of them occuring quite often in a given random product is small.
\end{example}
