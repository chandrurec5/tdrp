%!TEX root =  lsa.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{sec:discussion}
General discussion + step-size dependence on problem instances

\paragraph{Bias and Variance:} The means-squared error at $t$ is bounded by a sum of two terms. The first term is the bias term given by $\B=(1+4(\alpha\rho_{\alpha})^{-1}) (\alpha\rho_{\alpha})^{-1} \Big(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}\Big)$.  The \emph{Bias} term that captures the rate at which the initial condition $\norm{\theta_t-\ts}^2$ is forgotten. The second term is the \emph{Variance} given by $\V=(1+4(\alpha\rho_{\alpha})^{-1}) (\alpha\rho_{\alpha})^{-1} \Big(\frac{\alpha^2\sigma^2}{t+1} \Big)$. The variance term that captures the rate at which noise is rejected. The $\B$ and $\V$ terms capture two different sources of errors. To see this, note that in the absence of noise i.e., when $\zeta_t=0,~\forall t\geq 0$, we have $\E[\norm{H\eb_t}^2]=\B$ and in the presence of noise, i.e., $\zeta\neq 0,\forall t\geq 0$ and the perfect initial condition, i.e., $\theta_t=\ts$, we have $\E[\norm{H\eb_t}^2]=\V$.

\paragraph{Faster Rates:} The bias term term decays $O(1/t^2)$ and the variance term decays $O(1/t)$, which are faster than the rates that can be achieved by a diminishing step size sequence recommended by standard stochastic approximation theory \cite{SA}. In particular, when the step sizes are decaying $O(1/t)$ one can only achieve an rate that is $O(1/t^\mu)$ where $\mu$ is the smallest real part of the eigenvalue of $H$. Thus the choice of a constant step size sequence with iterate averaging on top helps us to eliminate the effect of `ill' conditioning of $H$.

\paragraph{Problem Dependent terms:} The choice of the step size is unfortunately problem dependent. To see this, the condition that $\rho_{\alpha}>0$ in \Cref{maintheorem} ensures that the expected spectral norm of $\E[(I-\alpha H_t)^\top(I-\alpha H_t)]$ is less than unity. And thus the range of $\alpha$ changes from problem to problem, an issue we further elaborate in \Cref{sec:stepprob}.

\paragraph{Behaviour for extreme value of $\alpha$:} For smaller values of step size, i.e., $\alpha\approx 0$, the bias term blows up, due to the presence of $\alpha^{-1}$ term. This is due to the fact that the step sizes determine the learning rate and for smaller step sizes the learning rate is slower. However, in this case the noise term does not blow up, a fact that can appreciated by looking at \eqref{lsergen} where $\alpha$ is seen to multiply the noise term $\zeta_t$. In quantitative terms, we can see that the $\alpha^{-2}$ and $\alpha^2$ terms can each other. For larger values of $\alpha$ i.e., $\alpha\ra \alpha_{\max}$, the bounds blow up again, due to the fact that $\rho_{\alpha}\ra 0$ in this case. This is due to the fact that the effect of both noise and initial conditions decay with the contraction factor of $F_{t,i}$, which gets closer to unity as $\alpha\ra\alpha_{\max}$.


\subsection{Step Size dependence on Problem Instance}
\label{sec:stepprob}
We discussed in \Cref{sec:prob} that the maximum step size depends on the problem instance. The idea is to choose the step size that ensures stability of the recursion, which in turn translates to contraction properties of the product of random matrices. We now elaborate on two notions of contractions namely $(i)$ contraction per step, $(ii)$ contraction of the product.
\paragraph{Contraction Per Time Step}
The proof of \Cref{maintheorem} uses the contraction per time step property. In such an analysis, the existence of problem independent $\alpha$ amounts the following formal condition stated in the form of \Cref{steptheorem}.
\begin{lemma}[Per Step Contraction]\label{steptheorem}
Let $\H$ be the space of $n\times n$ real matrices $H=(H_{ij}),i,j=1,\ldots,n$ with the absolute values of their entries bounded, i.e., $|H_{ij}|\leq B$ for some constant $B>0$. Let $\P$ be the space of distributions supported on $\M$ such that $~\forall x\in R^n, P\in \P,\E_{P}[x^\top H x] >0$ (where $H\in\H$ ). Then there exists an $\alpha_{\max}>0$ such that $\alpha\in(0,\alpha_{\max})$ implies $\us{\sup}{\norm{x}\leq 1,P\in\P}\E_{P}[\norm{(I-\alpha H)x}]< 1$.
\end{lemma}
\begin{example}[Counter Example for \Cref{steptheorem}]
Consider $\H=[-1,1]$ and $\P$ supported on $\{-1,1\}$ with $P(1)=\frac{1}{2}+\epsilon$ and $P(-1)=\frac{1}{2}-\epsilon$. Now we want translate the condition $\E_{P}[\norm{(I-\alpha H)x}]<1$
\begin{align}
\begin{split}
1-2\alpha\E[H]+\alpha^2 \E[H^2]<1\\
4\epsilon-\alpha>0\\
\end{split}
\end{align}
Since $\epsilon$ can be made arbitrarily close to $0$, $\alpha_{\max}=0$.
\end{example}
We now a case where $\alpha_{\max}>0$  can be achieved by restricting the support of $\P$ in \Cref{steptheorem}.
\begin{example}\label{rankonestep}
Let be $\H$ as in \Cref{steptheorem}, but define $\hat{\P}$ be the space of probability distributions supported only on rank-$1$ matrices expressed as $uu^\top$ for some $u\in \R^n$. Then there exists $\alpha_{\max}>0$ such that $\alpha\in(0,\alpha_{\max})$ implies $\us{\sup}{\norm{x}\leq 1,P\in\hat{\P}}\E_{P}[\norm{(I-\alpha H)x}]< 1$.\\
We now check for the condition as follows $\E_{P}[\norm{(I-\alpha H)x}]<1$
\begin{align}
\begin{split}
\E[x^\top(I-2\alpha H+\alpha^2 H^\top H)x]<1\\
\E[x(2 H- \alpha H^\top H)x]>0
\end{split}
\end{align}
We can now use the restriction on $\hat{\P}$, by observing that
\begin{align}
\E[x^\top H^\top Hx ]=\E[x^\top u^\top u u^\top u x]\leq B^2\E[x^\top H x]
\end{align}
Thus
\begin{align}
\begin{split}
\E[x(2 H- \alpha H^\top H)x]>2\E[x^\top H x]- B^2\alpha \E[x^\top H^x]>0,
\end{split}
\end{align}
Thus $\alpha_{\max}=\frac{2}{B^2}$.
\end{example}
\paragraph{Contraction over time $t$}
The condition of contraction per time step is overly restrictive in the general case (other than the one presented in \Cref{rankonestep}). We can relax this condition and ask whether a weaker result in \Cref{multheorem} holds.
\begin{lemma}\label{multheorem}
Let $\H$ be the space of $n\times n$ real matrices $H=(H_{ij}),i,j=1,\ldots,n$ with the absolute values of their entries bounded, i.e., $|H_{ij}|\leq B$ for some constant $B>0$. Let $\P$ be the space of distributions supported on $\M$ such that $~\forall x\in R^n, P\in \P,\E_{P}[x^\top H x] >0$ (where $H\in\H$ ). Then there exists an $\alpha_{\max}>0$ such that $\alpha\in(0,\alpha_{\max})$ implies $\us{\sup}{\norm{x}\leq 1,P\in\P,t\geq 0}\E_{P}[\norm{(I-\alpha H)^tx}]< \infty$.
\end{lemma}
\textbf{Possible Counter example for \Cref{multheorem} in $\R$}
Consider $\H=[-1,1]$ and $\P$ supported on $\{-1,1\}$ with $P(1)=\frac{1}{2}+\epsilon$ and $P(-1)=\frac{1}{2}-\epsilon$. Note that
\begin{align*}
\E[\Pi_{i=1}^t (1-\alpha H_i)]&=\exp{\ous{\sum}{i=1}{t}\log(1-\alpha H_i)}\\
&\approx \exp{t\E[\log(1-\alpha H_i)]}
\end{align*}
Now the condition $\E[\Pi_{i=1}^t (1-\alpha H_i)]<\infty,$ where $H_i\sim P$ loosely translates to the following
\begin{align*}
\E[\log(1-\alpha H_i)]<0,
\end{align*}
which can be verified by noting that
\begin{align*}
&\E[\log(1-\alpha H_i)]\\&=\E[(\frac{1}{2}+\epsilon)\log (1-\alpha)+(\frac{1}{2}-\epsilon)\log (1+\alpha)]\\
&=\frac{1}{2}\log(1-\alpha^2)+\epsilon(\log(1-\alpha)-\log(1+\alpha))\\
&<0
\end{align*}
\textbf{Possible Counter example for \Cref{multheorem} in $\R^2$}
Given any $\alpha$ the aim is to choose a distribution $P$ over $\H$ such that $\E_{P}[\norm{(I-\alpha H)^tx}]$ grows with $t$. In other words, given an $\alpha>0$, we are looking for a distribution $P'$ supported in the $\alpha$ neighbourhood of $I$ denoted by $\F$ such that $\E_{P'}[F^t]$ grows with $t$ (where $F\sim P'$). We now look at a possible suggestion for such a $P'$.\par
\begin{example}
Pick $F_1=\begin{bmatrix} 0 & 1-\beta_2 \\ 1+\beta_1& 0\\\end{bmatrix}^{\frac{1}{p}}$, $F_2=\begin{bmatrix} 0 & 1+\beta_1 \\ 1-\beta_2& 0\\\end{bmatrix}^{\frac{1}{p}}$, where $\beta_2>\beta_1$ such that $1+\frac{1}{2}(\beta_1-\beta_2)$. Let $P'$ be supported on $\{F_1,F_2\}$ with $P'(F_1)=\frac{1}{2}$ and $P'(F_2)=\frac{1}{2}$, and  $p$ is sufficiently large such that $F_1,F_2$ belong to the $\alpha$-neighbourhood of $I$. A possible way the random product can blow up is when products $F_2^pF_1^p$ occur in a contiguous stream. To see this,
\begin{align}
F_2^pF_1^p=\begin{bmatrix} (1+\beta_1) &0  \\ 0& (1-\beta_2)^2\\\end{bmatrix}
\end{align}
Thus $\norm{F_2^p F_1^p}>1$. However, such (i.e., $F_2^p F_1^p$) sequences have probability of occurance $\frac{1}{2^{2p}}$ is a sequence of length $2p$. Thus probability of them occuring quite often in a given random product is small.
\end{example}
