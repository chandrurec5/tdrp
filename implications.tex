\section{Implication of the Results}
\emph{Temporal Difference} (TD) learning algorithms \cite{} are widely used to learn value functions. A desirable feature of these algorithms that they are LSA algorithms that perform only $O(n)$ computations per time step. The vanilla TD (or simply TD) algorithm was first introduced in \cite{}. An unresolved issue for quite some time was the divergent behavior of TD in the \emph{off-policy} setting, where the sample were obtained from a policy different from the one whose value function needs to be computed. The Gradient Temporal Difference (GTD) learning algorithm \cite{} addressed the issue of divergence and are provably stable in \emph{off-policy} scenarios.
%Thus the GTD methods solve \eqref{linsys} in the general case when $\pi_t\neq\pi_b$.
%The \ofp convergence issue has also been addressed differently by the least-squares TD (LSTD) algorithm, which constructs $A$ and $b$ in \eqref{lsa}, and obtain $\theta^*=A^{-1}b$. As a consequence of the %matrix inversions involved, the LTSD performs a minimum of $O(n^2)$ computations, which can be a possible downside when $n$ is large.\par
Recently, newer variants of GTD namely, projected GTD2 and GTD-\emph{Mirror-Prox} were proposed in \cite{}. The authors in \cite{} observe that the GTD algorithm can be derived as a true stochastic gradient algorithms with repsect to a primal-dual saddle point objective function. This observation enables application of finite-time results for stochastic gradient (SG) algorithms to the GTD algorithm and at the same time, derive the newer variants based on the stochastic \emph{Mirror-Prox} algorithm \cite{}.\par
While several variants of TD algorithms have been developed, and have been empirically tested, as far as we gather, there are a number of poorly understood aspects of these algorithms, which we list as under.
\begin{itemize}[leftmargin=*]
\item Stochastic approximation \cite{SA} theory typically needs diminishing and square summable step-sizes. Further, SA theory deals with study of ordinary differnetial equations (ODEs) and lead only to asymptotic rates. The diminishing nature of the step-size schedules results in poor asymptotics, with the rates of convergence depending on the spectrum of the matrices involved in the SA updates. This is undesirable since the matrix (see \eqref{linsys}) involves quantities which $i$) are unkown and $ii$) vary across environment.
\item It is quite common in literature to sweep the step-size to choose the right setting that results in best performance or sometimes even to achieve a convergent behavior \cite{}.
\item It has been observed in experiments that the GTD is slow. While its variant GTD2 is faster than GTD, it is still slower compared to TD in the \onp setting. While, it is fine to draw understanding from experiments, it is however necessary to tie such observations to a theoretical phenomenon in a principled manner.
\item The GTD-MP is based on the SMP algorithm, which as \cite{} notes is especially effective when the original non-smooth function admits a smooth saddle-point representation. However, in the case of GTD the original as well as the saddle-point objective functions are, and hence the intuition why GTD-MP is effective in the GTD setting is absent.
\end{itemize}
Almost all the TD algorithms (except the LSTD) perform only $O(n)$ computations per time step and their updates are linear in $\theta$. Hence, we feel that it makes sense to study them (TD and GTD algorithms) under the framework of linear stochastic approximation (LSA) schemes. Acknowledging the linear nature of the updates helps us to obtain a qualitative and quantitative understanding of the performance of the various TD algorithms. In particular, the spectral nature of the matrices involved in the linear updates dictate the behavior of these algorithm. For example, it is well known that the divergence of TD in the \ofp setting can be directly attributed to the fact that the $A$ matrix cannot be ensured to have all eigen values with strictly positive real parts. Also, the LSA framework helps throws light on the fact that finite time behavior of these algorithms involves understanding products of random matrices (an aspect unnoticed in TD literature).\par
In this paper, we make the following specific contributions
\begin{itemize}[leftmargin=*]
\item \textbf{Towards TD:} We study the vanilla TD algorithm with constant step-size and Rupert-Polyak (CSRP) iterate averaging. Specifically, we study the behavior of $\theta_{t+1}=\theta_t+\alpha (b_t-A_t)$, where $b_t$ and $A_t$ are noisy samples of $b$ and $A$ (in \eqref{linsys}) respectively. The finite time error is split into two terms namely the bias term (due to the initial condition) and the variance term (due to noise). We show that CSRP results in a finite time performance of $O(1/t^2)$ and $O(1/t)$ respectively for the bias and the variance terms and the constant do not depend on the condition number of $A$.
\item \textbf{Towards general LSAs}: We show a weaker result for general linear iterates of the form $\theta_{t+1}=\theta_t+\alpha (g(b_t,A_t)-H(A_t)\theta_t)$, where $g$ and $H$ are appropriate functions. This formulation helps us to study other LSA algorithms such as GTD, GTD-MP, iLSTD. Even in this setting, finite time performance of $O(1/t^2)$ and $O(1/t)$ respectively for the bias and the variance terms holds, however the constants have dependence on the condition number of $H(A)$.
\item \textbf{Towards Step-sizes and Stability:} We turn towards theory of product of random matrices to comment about conditions on the constant step size that leads to convergent algorithms. In paritcular, we show that are two distinct regimes, one being conservative with a smaller constant step size resulting in convergence of iterates in the mean square sense, and the other being relaxed with a relatively larger constant step size guaranteeing only asymptotic stability in high probability.
\item \textbf{Towards GTD:} We show that the GTD does not admit convergence in the mean square sense, due to block zero entries in the leading diagonal of the update matrix, result in a sub-system that can diverge. We tie the Mirror-Prox idea to \emph{Predictor-Corrector} method in numerical analysis, which enables us to show that the GTD-MP eliminates the divergence issue by ensuring positive entries in the leading diagonal. This provides the much needed intuition that was lacking with respect to why the Mirror-Prox idea makes a difference in the case of GTD. Further, the results for general LSA schemes applies to GTD-MP, resulting in finite-time performance bounds for the mean squared error.
\item \textbf{Towards iLSTD:} The incremental version of LSTD algorithm (iLSTD) in \cite{} is also an LSA algorithm. It follows that iLSTD does not converge in the \ofp setting even when LSTD does. We propose a stable version of iLSTD which converges in the \ofp setting.
\item \textbf{Towards Open Issues:} We note that the GTD-MP algorithm is incorrect when the samples are obtained from a single trajectory.
\end{itemize}
The key take aways of the contributions are listed as under.
\begin{itemize}[leftmargin=*]
\item  The CSRP minimizes the overhead on tuning the step-sizes, in that it is enough to choose a constant step-size without bothering about the entire sequence of step-sizes. This constant step size can be found out by experiments.
\item The distinct regimes of step-size implies that observing a handful of convergent trajectories in practice might not mean that the algorithm is converging in mean squared sense.
\item The `slowness' of the GTD (and its variants) can be easily observed by looking at the spectrum of the update equation.
\end{itemize}
In addition to the contributions listed above, our aim is to stitch together relvant theoretical and practical aspects of the TD algorithm with an aim to gain an overall perspective of what makes them tick. We also present some experiments on simple domains to illustrate intended the message.
\comment{\subsection{Related Work}
The analsyis in this work is inspired by a related work in \cite{bachaistats}, in which the authors consider the problem of linear prediction with the penalty function as quadratic loss under an i.i.d  (with respect to some unknown distribution) assumption on the data. The linear prediction problem in \cite{bachaistats} is solved by an stochastic gradient descent (SGD) algorithm which is an LSA algorithm as well.  An important difference in our case is that unlike the linear prediction setting, the matrices involved in the TD updates are not symmetric. We nevertheless show that analysis on the lines of \cite{bachaistats} still holds for the LSAs in the RL setting, thereby presenting an analysis under more generalized assumptions i.e., in the absence of symmetry.
}
