\begin{abstract}
Linear stochastic approximation (LSA) algorithms arise quite naturally in various applications such linear least squares problem, solution to large scale linear systems and temporal difference learning algorithms. LSA schemes are often employed to solve for a desired parameter from noisy observations. In this paper, we are in the setting of LSA algorithms that employ a constant step size with iterate averaging under the presence of multiplicative noise and are interested in studying the mean-squared error (MSE) of the averaged iterates with respect to the desired solution. Our study is motivated by the recent results for an important sub-class of our setting namely linear least squares problem  wherein ``amazing'' properties such as instance independent choice for the step size and instance independent rate of convergence of the MSE have been demonstrated. In this paper, we ask the question whether these ``amazing" properties hold in the general setting that we consider.\par
We show that in the setting considered step sizes cannot be chosen in an instance independent fashion. Futher, we show that while a rate of $O(\frac{1}{t})$ can be obtained for the MSE the constants are problem specific. Thus we observe that the ``amazing" properties that hold for the linear least squares problem do not hold in general. On the positive side, constant step size with iterate averaging is still an improvement over diminishing step size schemes which can yeild only a $O(\frac{1}{t^\mu})$ ($\mu<1$) rate for the MSE.
    
\end{abstract}

