%!TEX root =  lsa.tex
\begin{abstract}
Linear stochastic approximation (LSA) algorithms arise quite naturally in various applications such linear least-squares problem, solution to large scale linear systems and temporal difference learning algorithms. LSA schemes are often employed to solve for a desired parameter from noisy observations. In this paper, we are in the setting of LSA algorithms that employ a constant stepsize with iterate averaging under the presence of multiplicative noise and are interested in studying the mean-squared error (MSE) of the averaged iterates with respect to the desired solution. Our study is motivated by the recent results for an important sub-class of our setting namely linear least-squares problem  wherein ``amazing'' properties such as instance independent choice for the stepsize and instance independent rate of convergence of the MSE have been demonstrated. In this paper, we ask the question whether these ``amazing" properties hold in the general setting that we consider.\par
We show that in the setting considered stepsizes cannot be chosen in an instance independent fashion. Further, we show that while a rate of $O(\frac{1}{t})$ can be obtained for the MSE the constants are problem specific. Thus we observe that the ``amazing" properties that hold for the linear least-squares problem do not hold in general. On the positive side, constant stepsize with iterate averaging is still an improvement over diminishing stepsize schemes which can yield only a $O(\frac{1}{t^\mu})$ ($\mu<1$) rate for the MSE.
    
\end{abstract}

