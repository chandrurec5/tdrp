%!TEX root =  speedylearn.tex
\section{Introduction}
Linear stochasitc approximation algorithms are popular in machine learning, and they learn from noisy data by performing linear updates. The LSA algorithms incrementally update their iterates $\theta_t\in \R^n$ so that they converge to $\ts\in \R^n$ (a fixed point or extremum). Each LSA algorithm has its corresponding linear stochastic error recursion (LSER) that captures the evolution of the error $e_t\eqdef \theta_t-\ts$. The general form of an LSER can be given by
$
e_{t+1}=(I-\alpha_t H_t)e_t+ M_{t+1},
$
where $\alpha_t>0,t\geq0$ is the step-size schedule, $\{H_t\in \R^{n\times n},t\geq 0\}$ is a sequence of random matrices and $\{M_t\in \R^n,t\geq 0\}$ is a noise process. The matrices $H_t$ and noise $M_t$ vary across the LSA algorithms, i.e., each LSA algorithm has its own $H_t$s and $M_t$s.
In this paper, we wish to understand the effect the so called Rupert-Polyak averaging of iterates when a constant step-size ($\alpha_t=\alpha,~\forall t\geq 0 $) chosen. In particular, we are interested in the finite time performance of the quantity $\err(t)=\E[\parallel\bar{e}_t\parallel^2]$, where $\bar{e}_t\eqdef \frac{1}{t}\sum_{s=0}^{t-1}e_s$ is the average of the error sequence.\par
Our work is inspired by a related work in \cite{bachaistats}, in which the authors consider the problem of linear prediction with the penalty function as quadratic loss under an i.i.d  (with respect to some unknown distribution) assumption on the data. The linear prediction problem in \cite{bachaistats} is solved by an stochastic gradient descent (SGD) algorithm which is an LSA algorithm as well. In \cite{bachaistats}, the finite time error is split into two terms namely the bias term (due to the initial condition) and the variance term (due to noise). Rupert-Polyak (RP) averaging is revisited in \cite{bachaistats}, to show that, under a constant step-size rule with iterate averaging, a finite time performance of $O(1/t^2)$ and $O(1/t)$ respectively for the bias and the variance terms can be achieved. An important difference in our case is that unlike the linear prediction setting, the matrices $H_t$ are not symmetric in the RL setting.\par
The motivation for our study is the problem of \emph{value function} estimation in reinforcement learning (RL), wherein we are required to estimate the value function corresponding to a Markov reward process\footnote{Please refer \cite{BertB} for a detailed background}. A particular aspect of the RL setting is that the explicit model of the Markov reward process is not known and only data in the form of samples is made available. The problem of value function estimation boils down to solving a given linear sytem of equations, a goal achieved by LSA algorithms. As a result, many RL algorithms are LSA algorithms as well, and hence can be analyzed by studying their corresponding SLERs. In this paper, we consider three distinct LSAs to solve a given system of linear equations, namely the \emph{fixed-point} LSA (FP-LSA), the \emph{least-squares} LSA (LS-LSA) and the \emph{saddle-point} LSA (SP-LSA). The FP-LSA is based on the idea fixed points and does not involve gradient type updates.  The LS-LSA and SP-LSA are also stochastic gradient algorithms for the quadratic and saddle point objective functions (the gradients are linear for these objectives).

\begin{itemize}
\item We present the finite time bound for the averaged iterates of the LSER. The bound has two terms namely a bias term due to the inital condition $e_0\neq 0$ and a variance term due the noise process $W_{t}$. We present the results in two parts, first for the general case of non-symmetric $H_t$ and one for the case of symmetric $H_t$.
\item We then apply our results on LSERs to obtain specific bounds for the three cases of FP, LS and SP-LSA.

\comment{first consider LSAs where the expected update $g-H\theta_t$ does not correspond to a gradient of any function which we call \emph{fixed-point} LSA (FP-LSA). We then consider two different Stochastic-Gradient-LSA (SG-LSA) algorithms namely Least-Squares-LSA (LS-LSA) and Saddle-Point-LSA (SP-LSA) based respectively on the idea of minimizing the least square and the saddle point objectives. We present finite time performance analysis of all three cases. Our specific contributions are
\item The conditions that guarantee stability and convergence of FP-LSAs is given by stochastic approximation (SA) theory \cite{}. We show that a finite time performance of $O(1/t^2)$ and $O(1/t)$ continue to hold for the bias and the variance terms of $\err_M(t)$ even in the absence of symmetric operators. A downside is that the LS-LSA algorithm needs two independent samples per iteration which might not be realistic in applications.
\item We point out to the fact that the SP-LSA algorthim can potentially diverge. We fix this by suggesting a modified version of the SP-LSA algorithm based on the idea of \emph{Predictor-Corrector} methods known in numerical analysis. However, this fix relies on the access to two independent samples each iteration.
\item We turn towards the theory of product of random matrices to understand stability of constant step-size LSA. We observe that there exists two regimes for the choice of constant step-size. One being conservative with a smaller constant step size resulting in convergence of iterates in the mean square sense, and the other being relaxed with a relatively larger constant step size guaranteeing only asymptotic stability in high probability.
}
\end{itemize}
\comment{
The finite time performance of the LS-LSA corresponds to the linear least squares setting studied in \cite{}. We have included it here as well for the sake of completeness.
The paradigm of reinforcement learning (RL) \cite{} captures the automated learning of agents to act from sampled obtained via direct interaction with the environment. Formally, an action sequence is known as a policy denoted by $\pi$. An important sub-problem in RL is to learn the value of any given policy $\pi$, which is captured by a score/value function $V^\pi$. It is quite common in practice to have a linear representation of the value function i.e., letting $V^\pi=\Phi \theta$, where $\Phi$ is a feature matrix and $\theta^*$ is a weight vector. Another common scenario in RL, known as \emph{off-policy} learning, is encountered when the agent has collected samples using a \emph{behavior} policy $\pi_b$ and needs to learn the value of a \emph{target} policy $\pi_t$. The trivial scenario when $\pi_b=\pi_t$ is known as \emph{on-policy} learning.\par}
