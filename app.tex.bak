%!TEX root =  lsa.tex
\onecolumn
\section{Appendix: Proof of the main result}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section we provide the proof of \cref{maintheorem}.
The proof technique for \Cref{maintheorem} has been adopted from Appendix $B$ of \cite{bachharder} . However, there are some criticial differences which are listed below.
\begin{itemize}[leftmargin=*]
\item Linear least squares problem is considered in \cite{bachharder} and the random matrices involved are known to be symmetric positive definite. This enables the authors in \cite{bachharder} to define operators from the space of symmetric matrices to the space of symmetric matric matrices and carry out the computations making use of such operators. In \Cref{genlsa} the matrix is known only to be Loxley positive definite and the random matrices involved are not symmetric. Thus we cannot define appropriate linear operators, and instead we have resort to an analysis that makes use of only the expected norms of the random matrices involved.
\item Another significant difference is the assumption of structured noise in \cite{bachharder}, which does not apply in our case, simply due to the fact that we don't have any symmetric matrices at all in our scheme of things.
\end{itemize}

\begin{proof}
Define $F_{i,j}\eqdef (I-\alpha_i H_i)\ldots (I-\alpha_j H_j),~\forall i\geq j$ and $F_{i,j}\eqdef I,~\forall i<j$.
\begin{align*}
\eb_t&=\frac{1}{t+1}\ous{\sum}{i=0}{t}e_i\\
&=\frac{1}{t+1}(\ous{\sum}{i=0}{t} F_{i,1} e_0+ \alpha\ous{\sum}{i=1}{t} \ous{\sum}{k=i}{t} F_{k,i+1}  \zeta_i )
\end{align*}
\begin{align*}
\E[\norm{H\eb_t}^2]&=\E\ip{H\eb_t,H\eb_t}\\
&=\frac{1}{(t+1)^2}\E\ip{H\ous{\sum}{i=0}{t}e_i,H\ous{\sum}{i=0}{t}e_i}
\end{align*}
\begin{align*}
\E\ous{\sum}{i=0}{t}\ous{\sum}{j=0}{t}\ip{He_i,He_j}=\E\ous{\sum}{i=0}{t}\ip{He_i,He_i}+ 2\E\ous{\sum}{i=0}{t-1}\ous{\sum}{j=i+1}{t}\ip{He_i,He_j}.
\end{align*}

\begin{align}\label{inter}
\E\ous{\sum}{i=0}{t-1}\ous{\sum}{j=i+1}{t}\ip{He_i,He_j}
&=\E\ous{\sum}{i=0}{t-1}\ous{\sum}{j=i+1}{t}\ip{He_i,H\big[F_{j,i+1} e_i+\alpha\sum_{k=i+1}^j F_{j,k+1}\zeta_{k}\big]}\nn\\
&=\E\ous{\sum}{i=0}{t-1}\ous{\sum}{j=i+1}{t}\ip{He_i,H F_{j,i+1} e_i},~\text{from \Cref{noisecancel}}\nn\\
&=\E\ous{\sum}{i=0}{t-1}\ous{\sum}{j=i+1}{t}\ip{He_i,(I-\alpha H)^{j-i}H e_i},~\text{from \Cref{unroll}}\nn\\
&=2(\alpha\rho_{\alpha})^{-1}\E\ous{\sum}{i=0}{t-1}\ip{He_i,H e_i}\nn\\
&\leq2(\alpha\rho_{\alpha})^{-1}\E\ous{\sum}{i=0}{t}\ip{He_i,H e_i}\nn\\
\end{align}
\begin{align*}
 \E\ous{\sum}{i=0}{t}\ous{\sum}{j=0}{t}\ip{He_i,He_j}&=\E\ous{\sum}{i=0}{t}(1+4(\alpha\rho_{\alpha})^{-1})\ip{He_i,He_i}
\end{align*}
Now
\begin{align*}
\E\ip{He_i,He_i}=\E\ip{F_{i,1}e_0,F_{i,1}e_0}+\ous{\sum}{j=1}{i}\E\ip{F_{i,j+1}\zeta_j,F_{i,j+1}\zeta_j}
\leq (1-\alpha\rho_{\alpha})^i+ (\alpha\rho_{\alpha})^{-1}\sigma^2
\end{align*}
\begin{align*}
\E\ous{\sum}{i=0}{t}\ous{\sum}{j=0}{t}\ip{He_i,He_j}&\leq (1+4(\alpha\rho_{\alpha})^{-1}) (\alpha\rho_{\alpha})^{-1}(t\sigma^2+\norm{\theta_0-\ts}^2)
\end{align*}
\begin{align}
\E[\norm{H\eb_t}]\leq (1+4(\alpha\rho_{\alpha})^{-1}) (\alpha\rho_{\alpha})^{-1} \Big(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2\sigma^2}{t+1} \Big)
\end{align}
\end{proof}

\begin{lemma}\label{noisecancel}
For any $x_{i-1}\in \R^n$ that is $\F_{i-1}$ measurable and $\forall ~t > i$ if follows that $\E[x_i^\top F_{t,i+1}\zeta_{i}]=0$
\end{lemma}
\small
\begin{proof}
\begin{align*}
&\E[x_t^\top F_{i,t+1}\zeta_{i}]\\
%=\E\big[\E[ x_{i-1}^\top F_{i,t+1}\zeta_{i}|\F_t]\big]\\
=&\E\Bigg[\E\bigg[\E\Big[\E\big[ x_{i-1}^\top F_{t,i+1}\zeta_{i}|\F_{t}\big]|\F_{t-1}\Big]\ldots|\F_{i-1}\bigg]\Bigg]\\
=&\E\Bigg[ x_{i-1}^\top \E\bigg[\E\Big[\E\big[(I-\alpha H_t)|\F_{t}\big]|\ldots \F_{i}\Big]\zeta_{i}|\F_{i-1}\bigg]\Bigg]\\
=&\E\Bigg[ x_{i-1}^\top \E[(I-\alpha H)^{t-i+1}\zeta_{i}|\F_{i-1}]\Bigg]\\
=&\E\Bigg[ x_{i-1}^\top (I-\alpha H)^{t-i}\E\bigg[\zeta_{i}|\F_{i-1}\bigg]\Bigg]\\
=&0
\end{align*}
\end{proof}
\begin{lemma}\label{unroll}
For all $j>i$, $\E \ip{He_i,H F_{j,i+1} e_i}=\E\ip{He_i,(I-\alpha H)^{j-i}H e_i}$
\end{lemma}
\begin{proof}
\begin{align}
\E \ip{He_i,H F_{j,i+1} e_i}&=\E\big[ \E[\ip{He_i,H (I-\alpha H_j)\ldots (I-\alpha H_{i+1} ) e_i}|\F_{j-1}]\big]\\
&=\E[\ip{He_i,H (I-\alpha H)F_{j-1,i+1} e_i}]
\end{align}
The proof follows by subsequently taking conditions expectations with respect to $\F_{j-2},\ldots \F_{i}$.
\end{proof}

\todoc[inline]{These were marked as stashed changes. Choose whichever you want.}
\begin{proof}
W.l.o.g  $i>j$
\begin{align}
&\E[\ip{F_{t,i+1} \zeta_{i},F_{t,j+1} \zeta_{j}}]\nn\\
\label{diffindexinter}=&\E\big[\E[ \zeta^\top_{i}(I-\alpha H_{i+1})^\top\ldots(I-\alpha H_t)^\top(I-\alpha H_t)\ldots(I-\alpha H_{j+1}) \zeta_{j}|\F_{t-1}]\big]\\
=&\E\big[ \zeta^\top_{i}(I-\alpha H_{i+1})^\top\ldots\E[(I-\alpha H_t)^\top(I-\alpha H_t)|\F_{t-1}]\ldots(I-\alpha H_{j+1}) \zeta_{j}\big]\\
=&\E\big[ \zeta^\top_{i}(I-\alpha H_{i+1})^\top\ldots C\ldots(I-\alpha H_{j+1}) \zeta_{j}\big]\\
\label{diffindex}=&\E[ \sum_{l} G_l],
\end{align}
where $l$ is any index and $G_l$s in \eqref{diffindex} are terms obtained by expanding the product in \eqref{diffindexinter} using the fact that $H_t=H+M_{t},~\forall t\geq 0$. Note that $G_l$ is a product involving powers of $\alpha H$, matrices such as $M^\top_{q+1}$ and $M_{p+1}$ for some $ i\leq p<t, j\leq q < t$ and vectors $\zeta^\top_i$ and $\zeta^\top_j$.
For any given $G_l$ and let $r(l)$ be the largest index such that either $M^\top_{r(l)+1}$ or $M_{r(l)+1}$ or $\zeta^\top_{r(l)+1}$ is present in that term.
\begin{align*}
\E[ \sum_{l} G_l]=  \sum_{l} \E\big[\E[G_l|\F_{r(l)}]\big]=0
\end{align*}
\end{proof}



\textbf{Linear Stochastic Error Recursion (LSER)} The error dynamics for the LSA in \eqref{eq:lsa} i.e., the dynamics of $e_t\eqdef\theta_t-\ts$ can be written as follows:
\begin{align}\label{errprelim}
&\theta_t-\ts=\theta_{t-1}-\ts+\alpha(g_t -H_t(\theta_t-\ts+\ts)),\text{~or}\nn\\
&e_t=(I-\alpha H_t)e_{t-1}+(N_t-M_t\ts).
\end{align}
In what follows we consider what we call linear stochastic error recursion (LSER) given by
\begin{align}\label{lsergen}
e_t=(I-\alpha H_t)e_{t-1}+\alpha \zeta_t,
\end{align}
where $\zeta_t\eqdef (N_t-M_t\ts)$.



\begin{theorem}
Let $\H$ be the space of $n\times n$ matrices $H=(H_{ij})$with bounded entries i.e., $|H_{ij}|\leq B$, for some $B>0$. Let $\P$ be the space of probability distributions over rank-$1$ matrices of form $H=uv^\top$ such that $~\forall x\in \R^n \E_{P}[x^\top H x]>0$, where $P\in \P$ and $H\sim P$. Then there exists an $\alpha_{\max}>0$ such that 
$\sup_{\norm{x}\leq 1,P\in \P}\E_{P}[x^\top (2H - \alpha H^\top H)x ]>0$ whenever $\alpha\in (0,\alpha_{\max})$.
\end{theorem}

\begin{proof}
\begin{align}
\begin{split}
\E_{P}[x^\top (2H - \alpha H^\top H)x ]&=\E_{P}[( x^\top( 2uv^\top -\alpha uv^\top u v^\top) x ) ]\\
&=\E_{P}[( x^\top u \,\, v^\top x (2-\alpha v^\top u)) ]\\
&\geq\E_{P}[( x^\top u\,\, v^\top x (2-\alpha B^2)) ]\\
&=\E_{P}[( x^\top H x (2-\alpha B^2)) ]\\
&>0 \text{~whenever}\quad (\alpha<\frac{2}{B^2})
\end{split}
\end{align}
Thus $\alpha_{\max}=\frac{2}{B^2}$.
\end{proof}
