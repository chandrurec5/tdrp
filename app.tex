%!TEX root =  lsa.tex
\onecolumn
\section{Appendix: Proof of the main result}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section we provide the proof of \cref{maintheorem}.

The proof technique for \Cref{maintheorem} has been adopted from Appendix $B$ of \cite{bachharder} . However, there are some critical differences which are listed below.
\begin{itemize}
\item Linear least squares problem is considered in \cite{bachharder} and the random matrices involved are known to be symmetric positive definite. This enables the authors in \cite{bachharder} to define operators from the space of symmetric matrices to the space of symmetric matrices and carry out the computations making use of such operators. In \Cref{genlsa} the matrix is known only to be (laxly) positive definite, i.e., the lack of symmetry. 
Thus we cannot define appropriate linear operators, and instead we have resort to an analysis that makes use of only the expected norms of the random matrices involved.
\item Another significant difference is the assumption of structured noise in \cite{bachharder}, which does not apply in our case, simply due to the fact that we don't have any symmetric matrices at all in our scheme of things.
\end{itemize}



\begin{proof}
The error dynamics for the LSA in \eqref{eq:lsa1} i.e., the dynamics of $e_t\eqdef\theta_t-\ts$ can be written as 
$\theta_t-\ts=\theta_{t-1}-\ts+\alpha(g_t -H_t(e_{t-1}+\ts))$.
From $H\ts=g$ it follows that $H_t\theta^* = g_t + M_t\ts - N_t$. Defining 
\[
\zeta_t = N_t - M_t \ts, \qquad t\ge 1
\] and combining with the previous equality, we get
\begin{align}\label{lsergen}
e_t=(I-\alpha H_t)e_{t-1}+\alpha \zeta_t\,.
\end{align}
Iterating this gives
\begin{align*}
e_t 
& = (I-\alpha H_t) (I-\alpha H_{t-1}) e_{t-2} + \alpha (I-\alpha H_t) \zeta_{t-1} +\alpha \zeta_t \\
& \quad \vdots\\
& = (I-\alpha H_t) \cdots (I-\alpha H_1) e_0 + \alpha (I-\alpha H_t) \cdots (I-\alpha H_2) \zeta_1 \\
&  \qquad \qquad\qquad \qquad\qquad \qquad\,\,\, + \alpha (I-\alpha H_t) \cdots (I-\alpha H_3) \zeta_2\\
&  \quad \vdots \\
& \qquad \qquad\qquad \qquad\qquad \qquad\,\,\, + \alpha \zeta_t\,,
\end{align*}
which can be written compactly as
\begin{align*}
e_t = F_{t,1} e_0 + \alpha (F_{t,2} \zeta_1 + \dots + F_{t,t+1} \zeta_t )\,.
\end{align*}
Hence, $\eb_t \doteq \tb_t-\ts$ satisfies
\begin{align*}
\eb_t&=\frac{1}{t+1}\ous{\sum}{i=0}{t}e_i
=\frac{1}{t+1}\left\{\ous{\sum}{i=0}{t} F_{i,1} e_0+ \alpha \sum_{i=1}^{t} \left(\sum_{k=i}^{t} F_{k,i+1} \right)\zeta_i   \right\}\,,
\end{align*}
where in the second sum we flipped the order of sums and swapped the names of the variables that the sum runs over.

It follows that \todoc{We should rather use $C$ instead of $H$ here?}
\begin{align*}
\E[\norm{H\eb_t}^2]&=\E\ip{H\eb_t,H\eb_t}
 =\frac{1}{(t+1)^2} \sum_{i,j=0}^t \E\ip{H e_i,He_j}\,.
\end{align*}
Hence, we see that it suffices to bound $\EE{\ip{H e_i, H e_j }}$.
\if0
\begin{align*}
\E\ous{\sum}{i=0}{t}\ous{\sum}{j=0}{t}\ip{He_i,He_j}=\E\ous{\sum}{i=0}{t}\ip{He_i,He_i}+ 2\E\ous{\sum}{i=0}{t-1}\ous{\sum}{j=i+1}{t}\ip{He_i,He_j}.
\end{align*}
\fi
There are two cases depending on whether $i=j$. When $i< j$,
\begin{align*}
\E\ip{He_i,He_j}
&=\E \ip{He_i,H\big[F_{j,i+1} e_i+\alpha\textstyle\sum_{k=i+1}^j F_{j,k+1}\zeta_{k}\big]}\\
&=\E\ip{He_i,H F_{j,i+1} e_i} & \text{(from \Cref{noisecancel})}\\
&=\E\ip{He_i,(I-\alpha H)^{j-i}H e_i}\,, &\text{(from \Cref{unroll})}
\end{align*}
and therefore
\begin{align*}
\label{inter}
\sum_{i=0}^{t-1}\sum_{j=i+1}^t \E\ip{ He_i,H e_j}
&=\frac2{\alpha\rho_{\alpha}} \ous{\sum}{i=0}{t-1}\E\ip{He_i,H e_i}
\leq \frac2{\alpha\rho_{\alpha}}\ous{\sum}{i=0}{t}\E\ip{He_i,H e_i}\,.
\end{align*}
Since $\sum_{i,j}\cdot{} = \sum_{i=j}\cdot{} + 2 \sum_i \sum_{j>i} \cdot{}$,
\begin{align*}
\ous{\sum}{i=0}{t}\ous{\sum}{j=0}{t} \E\ip{He_i,He_j}&= \left(1+\frac4{\alpha\rho_{\alpha}}\right)\ous{\sum}{i=0}{t}\E\ip{He_i,He_i}\,.
\end{align*}
Now, \todoc{So where did the next inequality come from?}
\begin{align*}
\E\ip{He_i,He_i}=\E\ip{F_{i,1}e_0,F_{i,1}e_0}+\ous{\sum}{j=1}{i}\E\ip{F_{i,j+1}\zeta_j,F_{i,j+1}\zeta_j}
\leq (1-\alpha\rho_{\alpha})^i+ \frac{\sigma^2}{\alpha \rho_\alpha}\,,
\end{align*}
and so
\begin{align*}
\ous{\sum}{i=0}{t}\ous{\sum}{j=0}{t} \E\ip{He_i,He_j}
&\leq \left(1+\frac4{\alpha\rho_{\alpha}}\right)\, \frac1{\alpha\rho_{\alpha}}\, (t\sigma^2+\norm{\theta_0-\ts}^2)\,.
\end{align*}
Putting things together,
\begin{align}
\E[\norm{H\eb_t}]
\leq \left(1+\frac4{\alpha\rho_{\alpha}}\right)\, \frac1{\alpha\rho_{\alpha}}\, \,
		\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2\sigma^2}{t+1} \right)\,.
\end{align}
\end{proof}

\begin{lemma}\label{noisecancel}
For any $x_{i-1}\in \R^n$ that is $\F_{i-1}$ measurable and $\forall ~t > i$ if follows that $\E[x_i^\top F_{t,i+1}\zeta_{i}]=0$
\end{lemma}
\small
\begin{proof}
\begin{align*}
&\E[x_t^\top F_{i,t+1}\zeta_{i}]\\
%=\E\big[\E[ x_{i-1}^\top F_{i,t+1}\zeta_{i}|\F_t]\big]\\
=&\E\Bigg[\E\bigg[\E\Big[\E\big[ x_{i-1}^\top F_{t,i+1}\zeta_{i}|\F_{t}\big]|\F_{t-1}\Big]\ldots|\F_{i-1}\bigg]\Bigg]\\
=&\E\Bigg[ x_{i-1}^\top \E\bigg[\E\Big[\E\big[(I-\alpha H_t)|\F_{t}\big]|\ldots \F_{i}\Big]\zeta_{i}|\F_{i-1}\bigg]\Bigg]\\
=&\E\Bigg[ x_{i-1}^\top \E[(I-\alpha H)^{t-i+1}\zeta_{i}|\F_{i-1}]\Bigg]\\
=&\E\Bigg[ x_{i-1}^\top (I-\alpha H)^{t-i}\E\bigg[\zeta_{i}|\F_{i-1}\bigg]\Bigg]\\
=&0
\end{align*}
\end{proof}
\begin{lemma}\label{unroll}
For all $j>i$, $\E \ip{He_i,H F_{j,i+1} e_i}=\E\ip{He_i,(I-\alpha H)^{j-i}H e_i}$
\end{lemma}
\begin{proof}
\begin{align}
\E \ip{He_i,H F_{j,i+1} e_i}&=\E\big[ \E[\ip{He_i,H (I-\alpha H_j)\ldots (I-\alpha H_{i+1} ) e_i}|\F_{j-1}]\big]\\
&=\E[\ip{He_i,H (I-\alpha H)F_{j-1,i+1} e_i}]
\end{align}
The proof follows by subsequently taking conditions expectations with respect to $\F_{j-2},\ldots \F_{i}$.
\end{proof}

\todoc[inline]{These were marked as stashed changes. Choose whichever you want.}
\begin{proof}
W.l.o.g  $i>j$
\begin{align}
&\E[\ip{F_{t,i+1} \zeta_{i},F_{t,j+1} \zeta_{j}}]\nn\\
\label{diffindexinter}=&\E\big[\E[ \zeta^\top_{i}(I-\alpha H_{i+1})^\top\ldots(I-\alpha H_t)^\top(I-\alpha H_t)\ldots(I-\alpha H_{j+1}) \zeta_{j}|\F_{t-1}]\big]\\
=&\E\big[ \zeta^\top_{i}(I-\alpha H_{i+1})^\top\ldots\E[(I-\alpha H_t)^\top(I-\alpha H_t)|\F_{t-1}]\ldots(I-\alpha H_{j+1}) \zeta_{j}\big]\\
=&\E\big[ \zeta^\top_{i}(I-\alpha H_{i+1})^\top\ldots C\ldots(I-\alpha H_{j+1}) \zeta_{j}\big]\\
\label{diffindex}=&\E[ \sum_{l} G_l],
\end{align}
where $l$ is any index and $G_l$s in \eqref{diffindex} are terms obtained by expanding the product in \eqref{diffindexinter} using the fact that $H_t=H+M_{t},~\forall t\geq 0$. Note that $G_l$ is a product involving powers of $\alpha H$, matrices such as $M^\top_{q+1}$ and $M_{p+1}$ for some $ i\leq p<t, j\leq q < t$ and vectors $\zeta^\top_i$ and $\zeta^\top_j$.
For any given $G_l$ and let $r(l)$ be the largest index such that either $M^\top_{r(l)+1}$ or $M_{r(l)+1}$ or $\zeta^\top_{r(l)+1}$ is present in that term.
\begin{align*}
\E[ \sum_{l} G_l]=  \sum_{l} \E\big[\E[G_l|\F_{r(l)}]\big]=0
\end{align*}
\end{proof}






\begin{theorem}
Let $\H$ be the space of $n\times n$ matrices $H=(H_{ij})$with bounded entries i.e., $|H_{ij}|\leq B$, for some $B>0$. Let $\P$ be the space of probability distributions over rank-$1$ matrices of form $H=uv^\top$ such that $~\forall x\in \R^n \E_{P}[x^\top H x]>0$, where $P\in \P$ and $H\sim P$. Then there exists an $\alpha_{\max}>0$ such that 
$\sup_{\norm{x}\leq 1,P\in \P}\E_{P}[x^\top (2H - \alpha H^\top H)x ]>0$ whenever $\alpha\in (0,\alpha_{\max})$.
\end{theorem}

\begin{proof}
\begin{align}
\begin{split}
\E_{P}[x^\top (2H - \alpha H^\top H)x ]&=\E_{P}[( x^\top( 2uv^\top -\alpha uv^\top u v^\top) x ) ]\\
&=\E_{P}[( x^\top u \,\, v^\top x (2-\alpha v^\top u)) ]\\
&\geq\E_{P}[( x^\top u\,\, v^\top x (2-\alpha B^2)) ]\\
&=\E_{P}[( x^\top H x (2-\alpha B^2)) ]\\
&>0 \text{~whenever}\quad (\alpha<\frac{2}{B^2})
\end{split}
\end{align}
Thus $\alpha_{\max}=\frac{2}{B^2}$.
\end{proof}
