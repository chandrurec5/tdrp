%!TEX root =  lsa.tex
\onecolumn
\section{Appendix: Proof of the main result}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section we provide the proof of \cref{maintheorem}.

The proof technique for \cref{maintheorem} has been adopted from Appendix $B$ of \cite{bachharder} . However, there are some critical differences which are listed below.
\begin{itemize}
\item Linear least squares problem is considered in \cite{bachharder} and the random matrices involved are known to be symmetric positive definite. This enables the authors in \cite{bachharder} to define operators from the space of symmetric matrices to the space of symmetric matrices and carry out the computations making use of such operators. In \cref{genlsa} the matrix is known only to be (laxly) positive definite, i.e., the lack of symmetry. 
Thus we cannot define appropriate linear operators, and instead we have resort to an analysis that makes use of only the expected norms of the random matrices involved.
\item Another significant difference is the assumption of structured noise in \cite{bachharder}, which does not apply in our case, simply due to the fact that we don't have any symmetric matrices at all in our scheme of things.
\end{itemize}



\begin{proof}
The error dynamics for the LSA in \eqref{eq:lsa1} i.e., the dynamics of $e_t\eqdef\theta_t-\ts$ can be written as 
$\theta_t-\ts=\theta_{t-1}-\ts+\alpha(g_t -H_t(e_{t-1}+\ts))$.
From $H\ts=g$ it follows that $H_t\theta^* = g_t + M_t\ts - N_t$. Defining 
\[
\zeta_t = N_t - M_t \ts, \qquad t\ge 1
\] 
and combining with the previous equality, we get
\begin{align}\label{lsergen}
e_t=(I-\alpha H_t)e_{t-1}+\alpha \zeta_t\,.
\end{align}
Note that 
$
\EE{ \norm{\zeta_t}^2 } 
= \EE{ \norm{N_t}^2 } + (\ts)^\top \EE{ M_t^\top M_t }\ts \le \sigma_N^2 + (\ts)^\top \Sigma_M \ts 
= \sigma^2
$.
Iterating \eqref{lsergen}  gives
\begin{align*}
e_t 
& = (I-\alpha H_t) (I-\alpha H_{t-1}) e_{t-2} + \alpha (I-\alpha H_t) \zeta_{t-1} +\alpha \zeta_t \\
& \quad \vdots\\
& = (I-\alpha H_t) \cdots (I-\alpha H_1) e_0 + \alpha (I-\alpha H_t) \cdots (I-\alpha H_2) \zeta_1 \\
&  \qquad \qquad\qquad \qquad\qquad \qquad\,\,\, + \alpha (I-\alpha H_t) \cdots (I-\alpha H_3) \zeta_2\\
&  \quad \vdots \\
& \qquad \qquad\qquad \qquad\qquad \qquad\,\,\, + \alpha \zeta_t\,,
\end{align*}
which can be written compactly as
\begin{align}
\label{eq:etft}
e_t = F_{t,1} e_0 + \alpha (F_{t,2} \zeta_1 + \dots + F_{t,t+1} \zeta_t )\,,
\end{align}
where $F_{t,i} = (I-\alpha H_t) (I-\alpha H_{t-1}) \dots (I-\alpha H_i)$ when $t\ge i$ and we also let $F_{t,t+1}=I$, the identity matrix.
Hence, $\eb_t \doteq \tb_t-\ts$ satisfies
\begin{align*}
\eb_t&=\frac{1}{t+1}\ous{\sum}{i=0}{t}e_i
=\frac{1}{t+1}\left\{\ous{\sum}{i=0}{t} F_{i,1} e_0+ \alpha \sum_{i=1}^{t} \left(\sum_{k=i}^{t} F_{k,i+1} \right)\zeta_i   \right\}\,,
\end{align*}
where in the second sum we flipped the order of sums and swapped the names of the variables that the sum runs over.

It follows that \todoc{We should rather use $C$ instead of $H$ here?}
\begin{align*}
\E[\norm{H\eb_t}^2]&=\E\ip{H\eb_t,H\eb_t}
 =\frac{1}{(t+1)^2} \sum_{i,j=0}^t \E\ip{H e_i,He_j}\,.
\end{align*}
Hence, we see that it suffices to bound $\EE{\ip{H e_i, H e_j }}$.
There are two cases depending on whether $i=j$. When $i< j$,
\begin{align*}
\E\ip{He_i,He_j}
&=\E \ip{He_i,H\big[F_{j,i+1} e_i+\alpha\textstyle\sum_{k=i+1}^j F_{j,k+1}\zeta_{k}\big]}\\
&=\E\ip{He_i,H F_{j,i+1} e_i} & \text{(from \cref{noisecancel})}\\
&=\E\ip{He_i,H (I-\alpha H)^{j-i} e_i} & \text{(from \cref{lem:unroll})}\\
&=\E\ip{He_i,(I-\alpha H)^{j-i}H e_i}\,, &\text{(because $(I-\alpha H)$ and $H$ commute)}
\end{align*}
and therefore
\begin{align*}
\label{inter}
\sum_{i=0}^{t-1}\sum_{j=i+1}^t \E\ip{ He_i,H e_j}
&=\frac2{\alpha\rho_{\alpha}} \ous{\sum}{i=0}{t-1}\E\ip{He_i,H e_i}
\leq \frac2{\alpha\rho_{\alpha}}\ous{\sum}{i=0}{t}\E\ip{He_i,H e_i}\,.
\end{align*}
Since $\sum_{i,j}\cdot{} = \sum_{i=j}\cdot{} + 2 \sum_i \sum_{j>i} \cdot{}$,
\begin{align*}
\ous{\sum}{i=0}{t}\ous{\sum}{j=0}{t} \E\ip{He_i,He_j}&= \left(1+\frac4{\alpha\rho_{\alpha}}\right)\ous{\sum}{i=0}{t}\E\ip{He_i,He_i}\,.
\end{align*}
Expanding $e_i$ using \eqref{eq:etft} and then using \cref{innerproduct} and that by assumption $\EE{ \norm{\zeta_j}^2 } \le \sigma^2$ gives
\begin{align*}
\E\ip{He_i,He_i}=\E\ip{F_{i,1}e_0,F_{i,1}e_0}+\alpha^2\ous{\sum}{j=1}{i}\E\ip{ F_{i,j+1}\zeta_j, F_{i,j+1}\zeta_j}
\leq (1-\alpha\rho_{\alpha})^i\norm{e_0}^2+ \alpha^2\frac{\sigma^2}{\alpha \rho_\alpha}\,,
\end{align*}
and so
\begin{align*}
\ous{\sum}{i=0}{t}\ous{\sum}{j=0}{t} \E\ip{He_i,He_j}
&\leq \left(1+\frac4{\alpha\rho_{\alpha}}\right)\, \frac1{\alpha\rho_{\alpha}}\, (t\sigma^2+\norm{\theta_0-\ts}^2)\,.
\end{align*}
Putting things together,
\begin{align}
\E[\norm{H\eb_t}]
\leq \left(1+\frac4{\alpha\rho_{\alpha}}\right)\, \frac1{\alpha\rho_{\alpha}}\, \,
		\left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2\sigma^2}{t+1} \right)\,.
\end{align}
\end{proof}

Let $\F_t = \sigma( H_1,\dots,H_t, \zeta_1,\dots,\zeta_t ) $, with $\F_0$ the $\sigma$-field that holds all random variables.
\newcommand{\cF}{\F}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}[Product unroll lemma]\label{lem:genunroll}
Let $t>i\ge 1$, $x,y\in \R^n$ be $\F_{i}$-measurable random vectors. Then, 
\begin{align*}
\E[x^\top F_{t,i+1}y|\F_i]=x^\top (I-\alpha H)^{t-i} y\,.
\end{align*}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
By the definition of $F_{t,i+1}$, 
and because $F_{t-1,i+1} = (I-\alpha H_{t-1}) \dots (I-\alpha H_{i+1})$ is $\cF_{t-1}$-measurable,
as are $x$ and $y$,
\begin{align*}
 \EE{x^\top F_{t,i+1} y | \cF_{t-1} } = x^\top \EE{ (I-\alpha H_t) | \cF_{t-1} } F_{t-1,i+1} y
=x^\top  (I-\alpha H)  F_{t-1,i+1} y\,.
\end{align*}
By the tower-rule for conditional expectations and our measurability assumptions,
\begin{align*}
\EE{x^\top F_{t,i+1} y | \cF_{t-2} } 
=x^\top  (I-\alpha H)  \EE{F_{t-1,i+1} |\cF_{t-2}} y
= x^\top (I-\alpha H)^2 F_{t-2,i+1} y\,.
\end{align*}
Continuing this way we get
\begin{align*}
\EE{x^\top F_{t,i+1} y | \cF_{t-j} } 
= x^\top (I-\alpha H)^j F_{t-j,i+1} y\,, \quad j=1,2,\dots,t-i\,.
\end{align*}
Specifically, for $j=t-i$ we get
\begin{align*}
\EE{x^\top F_{t,i+1} y | \cF_{i} }  = x^\top (I-\alpha H)^{t-i} y\,.
\end{align*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}\label{noisecancel}
Let $t>i\ge 1$ and let $x$ be an $\F_{i-1}$-measurable random vector. Then,
$\E[x^\top F_{t,i+1}\zeta_{i}]=0$.
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
By \cref{lem:genunroll},
\begin{align*}
\EE{x^\top F_{t,i+1} \zeta_i | \cF_{i} }  = x^\top (I-\alpha H)^{t-i} \zeta_i\,.
\end{align*}
Using the tower rule,
\begin{align*}
\EE{x^\top F_{t,i+1} \zeta_i | \cF_{i-1} } 
 = x^\top (I-\alpha H)^{t-i}\EE{ \zeta_i | \cF_{i-1} }= 0\,.
\end{align*}
\end{proof}

\begin{lemma}\label{lem:unroll}
For all $t>i\ge 0$, $\E \ip{He_i,H F_{t,i+1} e_i}=\E\ip{He_i,H(I-\alpha H)^{t-i} e_i}$.
\end{lemma}
\begin{proof}
The lemma follows directly from \cref{lem:genunroll}. Indeed, 
$\theta_i$ depends only on $H_1,\dots,H_{i},g_1,\dots,g_{i}$, $\theta_i$ and so is $e_i$ $\cF_i$-measurable.
Hence, the lemma is applicable and implies that
\begin{align*}
\EE{ \ip{He_i, HF_{t,i+1} e_i} | \cF_i } = 
\EE{ \ip{He_i, H (I-\alpha H)^{t-i} e_i} | \cF_i }\,.
\end{align*}
Taking expectation of both sides gives the desired result.
\end{proof}

\begin{lemma}\label{innerproduct}
Let $i>j \ge 0$ and let $x\in \R^n$ be an $\F_j$-measurable random vector.
Then, 
\begin{align*}
\E\ip{F_{i,j+1}x,F_{i,j+1}x}\leq (1-\alpha \rho_\alpha)^{i-j}\E\norm{x}^2\,.
\end{align*}
\end{lemma}
\begin{proof}
Note that
$S_t\doteq \EE{ (I-\alpha H_t)^\top (I-\alpha H_t) | \F_{t-1} } 
= I - \alpha (H^\top + H) + \alpha^2 \EE{ H_t^\top H_t | \F_{t-1} }$.
Since $(g_t,H_t)_t$ is an independent sequence, $\EE{ H_t^\top H_t|\F_{t-1}} = \EE{ H_1^\top H_1 }$.
Now, using the definition of $\rho_\alpha$ from \eqref{eq:rhodef},
$\sup_{x:\norm{x}= 1} x^\top S_t x = 1 - \alpha \inf_{x:\norm{x}=1} x^\top (H^\top + H - \alpha \EE{H_1^\top H_1}) x 
= 1-\alpha \rho_\alpha$.
Hence,
\begin{align*}
\EE{\ip{F_{i,j+1}x,F_{i,j+1}x}|\F_{i-1} }
&= \EE{x^\top F_{i-1,j+1}^\top (I-\alpha H_i)^\top (I-\alpha H_i) F_{i-1,j+1} x\,|\,\F_{i-1}}\\
&=(x F_{i-1,j+1})^\top \, S_i \, F_{i-1,j+1} x\\
&\le (1-\alpha \rho_\alpha) \, \ip{ F_{i-1,j+1} x, F_{i-1,j+1} x} \\
& \le (1-\alpha \rho_\alpha)^2\, \ip{ F_{i-2,j+1} x, F_{i-2,j+1} x} \\
& \quad \vdots \\
& \le (1-\alpha \rho_\alpha)^{i-j}\, \norm{x}^2\,.
%&= \E[x^\top (I-\alpha H_{j+1})\ldots (I-\alpha H_i)^\top (I-\alpha H_i)\ldots (I-\alpha H_{j+1})x|\F_{i-1}]\\
%&=(1-\alpha\rho_{\alpha}) \E[x^\top (I-\alpha H_{j+1})\ldots (I-\alpha H_{i-1})^\top (I-\alpha H_{i-1})\ldots (I-\alpha H_{j+1})x]
\end{align*}
\end{proof}






\begin{theorem}
Let $\H$ be the space of $n\times n$ matrices $H=(H_{ij})$with bounded entries i.e., $|H_{ij}|\leq B$, for some $B>0$. Let $\P$ be the space of probability distributions over rank-$1$ matrices of form $H=uv^\top$ such that $~\forall x\in \R^n \E_{P}[x^\top H x]>0$, where $P\in \P$ and $H\sim P$. Then there exists an $\alpha_{\max}>0$ such that 
$\sup_{\norm{x}\leq 1,P\in \P}\E_{P}[x^\top (2H - \alpha H^\top H)x ]>0$ whenever $\alpha\in (0,\alpha_{\max})$.
\end{theorem}

\begin{proof}
\begin{align}
\begin{split}
\E_{P}[x^\top (2H - \alpha H^\top H)x ]&=\E_{P}[( x^\top( 2uv^\top -\alpha uv^\top u v^\top) x ) ]\\
&=\E_{P}[( x^\top u \,\, v^\top x (2-\alpha v^\top u)) ]\\
&\geq\E_{P}[( x^\top u\,\, v^\top x (2-\alpha B^2)) ]\\
&=\E_{P}[( x^\top H x (2-\alpha B^2)) ]\\
&>0 \text{~whenever}\quad (\alpha<\frac{2}{B^2})
\end{split}
\end{align}
Thus $\alpha_{\max}=\frac{2}{B^2}$.
\end{proof}
