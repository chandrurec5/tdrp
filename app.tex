%!TEX root =  lsa.tex
\onecolumn
\section{Appendix: Proof of the main result}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section we provide the proof of \cref{maintheorem}.
\begin{proof}
$\E[\norm{H\eb_t}^2]=\E\ip{H\eb_t,H\eb_t}=\frac{1}{(t+1)^2}\E\ip{H\ous{\sum}{i=0}{t}(\theta_i-\ts),H\ous{\sum}{i=0}{t}(\theta_i-\ts)}$.
$\E\ous{\sum}{i=0}{t}\ous{\sum}{j=0}{t}\ip{H(\theta_i-\theta^*),H(\theta_j-\theta^*)}=\E\ous{\sum}{i=0}{t}\ip{H(\theta_i-\theta^*),H(\theta_j-\theta^*)}+ 2\E\ous{\sum}{i=0}{t-1}\ous{\sum}{j=i+1}{t}\ip{H(\theta_i-\theta^*),H(\theta_j-\theta^*)}$. Now
\begin{align}\label{inter}
&\E\ous{\sum}{i=0}{t-1}\ous{\sum}{j=i+1}{t}\ip{H(\theta_i-\theta^*),H(\theta_j-\theta^*)}\nn\\
&=\E\ous{\sum}{i=0}{t-1}\ous{\sum}{j=i+1}{t}\ip{H(\theta_i-\theta^*),H\big[F_{j,i+1} (\theta_i-\theta^*)+\alpha\sum_{k=i+1}^j F_{j,k+1}\zeta_{k}\big]}\nn\\
&=\E\ous{\sum}{i=0}{t-1}\ous{\sum}{j=i+1}{t}\ip{H(\theta_i-\theta^*),H F_{j,i+1} (\theta_i-\theta^*)}\nn\\
&=\E\ous{\sum}{i=0}{t-1}\ous{\sum}{j=i+1}{t}\ip{H(\theta_i-\theta^*),H(I-\alpha H)^{j-i} (\theta_i-\theta^*)}\nn\\
&=2(\alpha\rho_{\alpha})^{-1}\E\ous{\sum}{i=0}{t-1}\ip{H(\theta_i-\theta^*),H (\theta_i-\theta^*)}\nn\\
&\leq2(\alpha\rho_{\alpha})^{-1}\E\ous{\sum}{i=0}{t}\ip{H(\theta_i-\theta^*),H (\theta_i-\theta^*)}\nn\\
\end{align}
 $\E\ous{\sum}{i=0}{t}\ous{\sum}{j=0}{t}\ip{H(\theta_i-\theta^*),H(\theta_j-\theta^*)}=\E\ous{\sum}{i=0}{t}(1+4(\alpha\rho_{\alpha})^{-1})\ip{H(\theta_i-\theta^*),H(\theta_i-\theta^*)}
$

Now
\begin{align*}
\E\ip{H(\theta_i-\theta^*),H(\theta_i-\theta^*)}=\E\ip{F_{i,1}e_0,F_{i,1}e_0}+\ous{\sum}{j=1}{i}\E\ip{F_{i,j+1}\zeta_j,F_{i,j+1}\zeta_j}
\leq (1-\alpha\rho_{\alpha})^i+ (\alpha\rho_{\alpha})^{-1}\sigma^2
\end{align*}
$\E\ous{\sum}{i=0}{t}\ous{\sum}{j=0}{t}\ip{H(\theta_i-\theta^*),H(\theta_j-\theta^*)}\leq (1+4(\alpha\rho_{\alpha})^{-1}) (\alpha\rho_{\alpha})^{-1}(t\sigma^2+\norm{\theta_0-\ts}^2)$
\begin{align}
\E[\norm{H\eb_t}]\leq (1+4(\alpha\rho_{\alpha})^{-1}) (\alpha\rho_{\alpha})^{-1} \Big(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+ \frac{\alpha^2\sigma^2}{t+1} \Big)
\end{align}
\end{proof}

\todoc[inline]{These were marked as stashed changes. Choose whichever you want.}
\begin{proof}
W.l.o.g  $i>j$
\begin{align}
&\E[\ip{F_{t,i+1} \zeta_{i},F_{t,j+1} \zeta_{j}}]\nn\\
\label{diffindexinter}=&\E\big[\E[ \zeta^\top_{i}(I-\alpha H_{i+1})^\top\ldots(I-\alpha H_t)^\top(I-\alpha H_t)\ldots(I-\alpha H_{j+1}) \zeta_{j}|\F_{t-1}]\big]\\
=&\E\big[ \zeta^\top_{i}(I-\alpha H_{i+1})^\top\ldots\E[(I-\alpha H_t)^\top(I-\alpha H_t)|\F_{t-1}]\ldots(I-\alpha H_{j+1}) \zeta_{j}\big]\\
=&\E\big[ \zeta^\top_{i}(I-\alpha H_{i+1})^\top\ldots C\ldots(I-\alpha H_{j+1}) \zeta_{j}\big]\\
\label{diffindex}=&\E[ \sum_{l} G_l],
\end{align}
where $l$ is any index and $G_l$s in \eqref{diffindex} are terms obtained by expanding the product in \eqref{diffindexinter} using the fact that $H_t=H+M_{t},~\forall t\geq 0$. Note that $G_l$ is a product involving powers of $\alpha H$, matrices such as $M^\top_{q+1}$ and $M_{p+1}$ for some $ i\leq p<t, j\leq q < t$ and vectors $\zeta^\top_i$ and $\zeta^\top_j$.
For any given $G_l$ and let $r(l)$ be the largest index such that either $M^\top_{r(l)+1}$ or $M_{r(l)+1}$ or $\zeta^\top_{r(l)+1}$ is present in that term.
\begin{align*}
\E[ \sum_{l} G_l]=  \sum_{l} \E\big[\E[G_l|\F_{r(l)}]\big]=0
\end{align*}
\end{proof}



\textbf{Linear Stochastic Error Recursion (LSER)} The error dynamics for the LSA in \eqref{eq:lsa} i.e., the dynamics of $e_t\eqdef\theta_t-\ts$ can be written as follows:
\begin{align}\label{errprelim}
&\theta_t-\ts=\theta_{t-1}-\ts+\alpha(g_t -H_t(\theta_t-\ts+\ts)),\text{~or}\nn\\
&e_t=(I-\alpha H_t)e_{t-1}+(N_t-M_t\ts).
\end{align}
In what follows we consider what we call linear stochastic error recursion (LSER) given by
\begin{align}\label{lsergen}
e_t=(I-\alpha H_t)e_{t-1}+\alpha \zeta_t,
\end{align}
where $\zeta_t\eqdef (N_t-M_t\ts)$.



\begin{theorem}
Let $\H$ be the space of $n\times n$ matrices $H=(H_{ij})$with bounded entries i.e., $|H_{ij}|\leq B$, for some $B>0$. Let $\P$ be the space of probability distributions over rank-$1$ matrices of form $H=uv^\top$ such that $~\forall x\in \R^n \E_{P}[x^\top H x]>0$, where $P\in \P$ and $H\sim P$. Then there exists an $alpha_{\max}>0$ such that $\ous{\sup}{\norm{x}\leq 1,P\in \P}\E_{P}[x^\top (2H - \alpha H^\top H)x ]>0$ whenever $\alpha\in (0,\alpha_{\max})$
\end{theorem}

\begin{proof}
\begin{align}
\begin{split}
\E_{P}[x^\top (2H - \alpha H^\top H)x ]&=\E_{P}[( x^\top 2uv^\top -\alpha uv^\top u v^\top x ) ]\\
&=\E_{P}[( x^\top 2uv^\top)x (2-\alpha v^\top u)) ]\\
&\geq\E_{P}[( x^\top uv^\top)x (2-\alpha B^2)) ]\\
&=\E_{P}[( x^\top H)x (2-\alpha B^2)) ]\\
&>0 \text{~whenever}\quad (\alpha<\frac{2}{B^2})
\end{split}
\end{align}
Thus $\alpha_{\max}=\frac{2}{B^2}$.
\end{proof}
