%!TEX root =  lsa.tex
\section{Problem Setup and Main Result}\label{sec:prob}
%\input{restable}
We consider constant stepsize linear stochastic approximation (LSA) algorithms with iterate averaging.
These algorithms update a pair of parameters $\theta_t,\tb_t\in \R^n$ incrementally, in discrete time steps $t=1,2,\dots$
based on data $g_t\in \R^n$, $H_t\in \R^{n\times n}$, using the updates
\begin{subequations}
\label{eq:lsa} % prefer eq:lsa etc.
\begin{align}
\theta_{t} & =\theta_{t-1}+\alpha\,(g_t-H_t\theta_{t-1})\,, \label{eq:lsa1}\\
 \tb_t & =  \frac{1}{t+1}\sum_{s=0}^{t} \theta_s\,, \label{eq:rp}
\end{align}
\end{subequations}
where $\alpha>0$ is a positive step-size parameter; the only tuning parameter of the algorithm besides the 
initial value $\theta_0$.
The iterate $\theta_t$ is treated as an internal state of the algorithm, while $\tb_t$ is the output at time step $t$.
The update of $\theta_t$ alone is considered a form of constant stepsize LSA.
Note that the computation of $\tb_t$ can also be performed in an incremental fashion with $O(n)$ storage.

Owning to the special form of $H_t$, sometimes the matrix-vector product $H_t \theta_{t-1}$ 
can be efficiently computed in $O(n)$ time.
This happens for example when $H_t$ is rank one; two specific examples of this kind are presented later in this section.
The significance of efficient computation of the matrix-vector product is that then an update of the algorithm
can be implemented in $O(n)$ time with $O(n)$ storage,
which makes the algorithm particularly attractive in large-scale computations when $n$ is in the range of thousands or more.

We are interested in the behaviour of \eqref{eq:lsa} under the following conditions:
%%%%%%%%%%%%%%%%%%%%%
\begin{assumption}\label{genlsa}
%%%%%%%%%%%%%%%%%%%%%
\mbox{}
\vspace*{-0.14in}

\begin{enumerate}
%%%%%%%%%%%%%%%%%%%%%
\item\label{mart} 
The random quantities $(H_t,g_t)_t$  are independent of each other with common means $\EE{ H_t} = H$ and $\EE{g_t} = g$.

%%%%%%%%%%%%%%%%%%%%%
\item \label{noise} The ``noise variables'', $M_t = H_t - H$ and $N_t = g_t - g$ have uniformly bounded second moments;
in particular, we let $\sigma_1^2>0$ and $\Sigma_2^2\succ 0$ such that for any $t\ge 1$,
$\sup_{t\ge 1} \E[N_t^\top N_t]<\sigma_1^2$ and
$\E[M_t^\top M_t]\preceq \Sigma_2^2$ both hold.

%%%%%%%%%%%%%%%%%%%%%
\item \label{mat} $H$ is positive definite (PD). % for all $x\in \R^n\setminus \{0\}$, $\ip{x,Hx}>0$. 
\end{enumerate}
\end{assumption}
<<<<<<< HEAD
Here, $\ts$ is a parameter to be estimated that the LSA in \eqref{lsa} aims to compute from noisy data presented in the form of $g_t$ and $H_t$.\par
Our motivation to study constant step size LSA with RP-averaging stems from the fact that LSAs are common in applications such as temporal difference learning algorithms \cite{td} in reinforcement learning (RL)\cite{rl}, solution to large scale linear systems \cite{bertsekas}, and the linear least squares problem in \cite{bachharder}. In particular, constant step size and RP averaging has been shown to have `` amazing" properties in the case of linear least squares problem \cite{bachharder} and we would like to investigate further as to whether these properties generalize to LSA.
\begin{example}[Temporal Difference Learning]
The Temporal Difference learning algorithm with a constant step size $\alpha>0$ is given by
\begin{align}\label{tdzero}
\theta_t=&\theta_{t-1}+\alpha \big(\phi(s_t) R(s_t)\nn\\&+ (\phi(s_t)\phi^\top(s_t)-\gamma \phi(s_t)\phi^\top(s'_t))\theta_{t-1}\big),
\end{align}
where are $s_t,s'_t\in S$ with $S$ being a finite set, $\phi(s)\in \R^n$ are the so called \emph{feature} vectors, $\gamma\in (0,1)$ is a given discount factor and $R\colon S\ra \R$ is map from S to reals. Here, $s_t$ are $i.i.d$ random variables distributed according to the law $s_t\sim \xi$ where $\xi$ is supported on $S$ and $s'_t\sim p(s_t,\cdot)$. The TD algorithm in \eqref{tdzero} can be cast in the general form as presented in \eqref{lsa}, by letting $g_t=\phi(s_t)R(s_t)$ and $H_t=\phi(s_t)\phi^\top(s_t)-\gamma \phi(s_t)\phi^\top(s'_t)$.
=======
Note that $H$ is not necessarily symmetric.
Since $H$ is PD, it is invertible. We let
\begin{align*}
\ts = H^{-1} g\,.
\end{align*}
%%%%%%%%%%%%%%%%%%%%%
The ``aim'' of the updates in \eqref{eq:lsa} is to estimate $\ts$ based on the noisy versions $(H_t,g_t)$ of $(H,g)$.
There are various ways of quantifying the error $\eb_t \doteq \tb_t - \ts$ of estimating $\ts$ using $\tb_t$, but the magnitude
of the errors is usually measured in some weighted quadratic norm. The expected loss for time step $t$ then becomes
\begin{align*}
L_t \doteq \EE{ \norm{\eb_t}_{C}^2 }\,,
\end{align*}
where recall that for a SPD matrix $C$, $\norm{x}_C^2 = x^\top C x$.
Here, the choice of the matrix $C$ depends on the application.


As noted in the introduction, LSA is widely used in science and engineering. Here, we provide two specific examples
of LSA, which we will use throughout the paper:
%%%%%%%%%%%%%%%%%%%%%
\begin{example}[Linear Prediction under Quadratic Loss]
Fill this in similarly to the TD example. What is the data, what is the goal (here we can explain briefly),
what is the update, what is the loss metric.
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
>>>>>>> origin/master
\end{example}
%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%
\begin{example}[Temporal Difference  (TD) Learning]
The simplest temporal differencelearning algorithm with a constant stepsize $\alpha>0$ is given by
\begin{align}
\label{tdzero}
\begin{split}
\theta_t
& = \theta_{t-1}+\alpha\, (R_t+ \gamma \ip{\theta_{t-1},\tilde{\phi}_t } - \ip{\theta_{t-1},\phi_t})\phi_t \\
& = \theta_{t-1}+\alpha\, \bigl( R_t\phi_t- (\phi_t \phi_t^\top-\gamma \phi_t \tilde{\phi}_t^\top)\theta_{t-1}\bigr)\,.
\end{split}
\end{align}
Here, $(\phi_t,\tilde{\phi}_t,R_t)\in \R^n\times \R^n \times \R$ is the data at time $t$.
Assuming $(\phi_t,\tilde{\phi}_t,R_t)$ is i.i.d. and say, bounded, 
we see that with $H_t = \phi_t \phi_t^\top-\gamma \phi_t \tilde{\phi}_t^\top$, $g_t = R_t \phi_t$ is in the form of 
\eqref{eq:lsa1} and satisfies the assumptions as long as $H = \EE{H_t}$ is PD. 
Note that the matrices $H_t$ are rank-1, hence the algorithm can indeed be implemented with $O(n)$ storage and
time (as is also shown in the first form of the update of $\theta_t$).
However, the matrices $H_t$ are not symmetric (nor is $H$ symmetric in general).
\todoc[inline]{Finish: This is indeed satisfied in ``on policy'' TD, etc. Add at least one reference. Note that this is TD(0).}
In TD learning the error of a parameter $\theta$ is commonly measured using the expected squared 
prediction error $L_t(\theta) = \EE{ (\phi_t^\top \theta - \phi_t^\top \ts)^2 } = \norm{\theta - \ts}_C^2$,
where $C = \EE{ \phi_t \phi_t^\top }$ is assumed to be PD.
\if0
\begin{align}\label{tdzero}
\theta_t=&\theta_{t-1}+\alpha \big(\phi(s_t) R(s_t)\nn\\&+ (\phi(s_t)\phi^\top(s_t)-\gamma \phi(s_t)\phi^\top(s'_t))\theta_{t-1}\big),
\end{align}
where are $s_t,s'_t\in S$ with $S$ being a finite set, $\phi(s)\in \R^n$ are the so called \emph{feature} vectors, $\gamma\in (0,1)$ is a given discount factor and $R\colon S\ra \R$ is map from S to reals. \todoc{Shoot, I wish we used $d$ in place of $n$..}
Here, $s_t$ are $i.i.d$ random variables distributed according to the law $s_t\sim \xi$ where $\xi$ is supported on $S$ and $s'_t\sim p(s_t,\cdot)$. The TD algorithm in \eqref{tdzero} can be cast in the general form as presented in \eqref{eq:lsa}, by letting $g_t=\phi(s_t)R(s_t)$ and $H_t=\phi(s_t)\phi^\top(s_t)-\gamma \phi(s_t)\phi^\top(s'_t)$.
\fi
\end{example}
%%%%%%%%%%%%%%%%%%%%%
The following theorem is the main result of the paper.
To state the result we define the key quantity
\begin{align*}
\rho_\alpha\eqdef \sup_{\norm{x}\leq 1} \E[x^\top (2H-\alpha H_tH_t)x]\,.
\end{align*}
\begin{theorem}\label{maintheorem}
Assume that $\alpha>0$ is such that $\rho_\alpha>0$. Then, it follows that for \emph{any} $t\ge 1$,
\begin{align}
\begin{split}
\MoveEqLeft\E[\norm{H(\tb_t-\ts)}^2]\leq \\
&\left(1+4\frac1{\alpha\rho_{\alpha}}\right) \frac{1}{\alpha\rho_{\alpha}}
 \left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+\frac{\alpha^2\sigma^2}{t+1} \right)\,.
\end{split}
\end{align}
\end{theorem}
<<<<<<< HEAD
<<<<<<< Updated upstream
=======

>>>>>>> Stashed changes
\textbf{Discussion}
\begin{enumerate}[leftmargin=*]
\item \textbf{Bias and Variance:} The means-squared error at $t$ is bounded by a sum of two terms. The first term is the bias term given by $\B=(1+4(\alpha\rho_{\alpha})^{-1}) (\alpha\rho_{\alpha})^{-1} \Big(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}\Big)$.  The \emph{Bias} term that captures the rate at which the initial condition $\norm{\theta_t-\ts}^2$ is forgotten. The second term is the \emph{Variance} given by $\V=(1+4(\alpha\rho_{\alpha})^{-1}) (\alpha\rho_{\alpha})^{-1} \Big(\frac{\alpha^2\sigma^2}{t+1} \Big)$. The variance term that captures the rate at which noise is rejected. The $\B$ and $\V$ terms capture two different sources of errors. To see this, note that in the absence of noise i.e., when $\zeta_t=0,~\forall t\geq 0$, we have $\E[\norm{H\eb_t}^2]=\B$ and in the presence of noise, i.e., $\zeta\neq 0,\forall t\geq 0$ and the perfect initial condition, i.e., $\theta_t=\ts$, we have $\E[\norm{H\eb_t}^2]=\V$.
\item \textbf{Faster Rates:} The bias term term decays $O(1/t^2)$ and the variance term decays $O(1/t)$, which are faster than the rates that can be achieved by a diminishing step size sequence recommended by standard stochastic approximation theory \cite{sa}. In particular, when the step sizes are decaying $O(1/t)$ one can only achieve an rate that is $O(1/t^\mu)$ where $\mu$ is the smallest real part of the eigenvalue of $H$. Thus the choice of a constant step size sequence with iterate averaging on top helps us to eliminate the effect of `ill' conditioning of $H$.
\item \textbf{Problem Dependent terms:} The choice of the step size is unfortunately problem dependent. To see this, the condition that $\rho_{\alpha}>0$ in \Cref{maintheorem} ensures that the expected spectral norm of $\E[(I-\alpha H_t)^\top(I-\alpha H_t)]$ is less than unity. And thus the range of $\alpha$ changes from problem to problem, an issue we further elaborate in \Cref{sec:stepprob}.
\item \textbf{Behaviour for extreme value of $\alpha$:} For smaller values of step size, i.e., $\alpha\approx 0$, the bias term blows up, due to the presence of $\alpha^{-1}$ term. This is due to the fact that the step sizes determine the learning rate and for smaller step sizes the learning rate is slower. However, in this case the noise term does not blow up, a fact that can appreciated by looking at \eqref{lsergen} where $\alpha$ is seen to multiply the noise term $\zeta_t$. In quantitative terms, we can see that the $\alpha^{-2}$ and $\alpha^2$ terms can each other. For larger values of $\alpha$ i.e., $\alpha\ra \alpha_{\max}$, the bounds blow up again, due to the fact that $\rho_{\alpha}\ra 0$ in this case. This is due to the fact that the effect of both noise and initial conditions decay with the contraction factor of $F_{t,i}$, which gets closer to unity as $\alpha\ra\alpha_{\max}$.
\end{enumerate}
The results of \Cref{maintheorem} can be extended to the case when $\zeta_t$ in \eqref{lser} is a martingale noise with the following assumptions.
\begin{assumption}
\begin{enumerate}
\item $\zeta_t\in \R^{n}$ is a martingale difference sequence with respect to a increasing sequence of $\sigma$-fields given by $\F_t\eqdef \sigma \{e_s, \zeta_s,s\leq t\}$, in particular, $\E[\zeta_t|F_{t-1}]=0,~\forall t\geq 0$.
\item \label{secondmom} $E[\zeta_t \zeta_t^\top| \F_{t-1}]=\Sigma^2$ for all $0\leq s< t$.
\end{enumerate}
\end{assumption}
The proof of \Cref{maintheorem} can be re-used after we establish the fact that in the expansion of $\E[\norm{H\eb_t}^2]$ the terms that involve inner products containing $\zeta_i$ and $\zeta_j$ for $i\neq j$ are zero. This is captured by \Cref{crosstermmart}
\begin{lemma}\label{crosstermmart}
For $i\neq j$, $\E[\ip{F_{t,i+1}\zeta_{i},F_{t,j+1} \zeta_{j}}]=0$
\end{lemma}
However, the result in \Cref{maintheorem} cannot be extended to the case of Markov noise which requires diminishing step-sizes \cite{prashanth,neurodyna}.
=======
\todoc[inline]{The result should rather be stated for $\EE{\norm{\tb_t-\ts}_C^2}$.}
>>>>>>> origin/master
