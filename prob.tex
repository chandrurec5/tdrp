%!TEX root =  lsa.tex
\section{Problem Setup and Main Result}\label{sec:prob}
%\input{restable}
We consider constant stepsize linear stochastic approximation (LSA) algorithms with iterate averaging.
These algorithms update a pair of parameters $\theta_t,\tb_t\in \R^n$ incrementally, in discrete time steps $t=1,2,\dots$
based on data $g_t\in \R^n$, $H_t\in \R^{n\times n}$, using the updates
\begin{subequations}
\label{eq:lsa} % prefer eq:lsa etc.
\begin{align}
\theta_{t} & =\theta_{t-1}+\alpha\,(g_t-H_t\theta_{t-1})\,, \label{eq:lsa1}\\
 \tb_t & =  \frac{1}{t+1}\sum_{s=0}^{t} \theta_s\,, \label{eq:rp}
\end{align}
\end{subequations}
where $\alpha>0$ is a positive step-size parameter; the only tuning parameter of the algorithm besides the 
initial value $\theta_0$.
The iterate $\theta_t$ is treated as an internal state of the algorithm, while $\tb_t$ is the output at time step $t$.
The update of $\theta_t$ alone is considered a form of constant stepsize LSA.
Note that the computation of $\tb_t$ can also be performed in an incremental fashion with $O(n)$ storage.

Owning to the special form of $H_t$, sometimes the matrix-vector product $H_t \theta_{t-1}$ 
can be efficiently computed in $O(n)$ time.
This happens for example when $H_t$ is rank one; two specific examples of this kind are presented later in this section.
The significance of efficient computation of the matrix-vector product is that then an update of the algorithm
can be implemented in $O(n)$ time with $O(n)$ storage,
which makes the algorithm particularly attractive in large-scale computations when $n$ is in the range of thousands or more.

We are interested in the behaviour of \eqref{eq:lsa} under the following conditions:
%%%%%%%%%%%%%%%%%%%%%
\begin{assumption}\label{genlsa}
%%%%%%%%%%%%%%%%%%%%%
\mbox{}
\vspace*{-0.14in}

\begin{enumerate}
%%%%%%%%%%%%%%%%%%%%%
\item\label{mart} 
The random quantities $(H_t,g_t)_t$  are independent of each other with common means $\EE{ H_t} = H$ and $\EE{g_t} = g$.

%%%%%%%%%%%%%%%%%%%%%
\item \label{noise} The ``noise variables'', $M_t = H_t - H$ and $N_t = g_t - g$ have uniformly bounded second moments;
in particular, we let $\sigma_N^2>0$ and $\Sigma_M^2\succ 0$ such that for any $t\ge 1$,
$\sup_{t\ge 1} \E[N_t^\top N_t]<\sigma_N^2$ and
$\E[M_t^\top M_t]\preceq \Sigma_M^2$ both hold.

%%%%%%%%%%%%%%%%%%%%%
\item \label{mat} $H$ is positive definite (PD). % for all $x\in \R^n\setminus \{0\}$, $\ip{x,Hx}>0$. 
\end{enumerate}
\end{assumption}
Note that $H$ is not necessarily symmetric.
Since $H$ is PD, it is invertible. We let
\begin{align*}
\ts = H^{-1} g\,.
\end{align*}
%%%%%%%%%%%%%%%%%%%%%
The ``aim'' of the updates in \eqref{eq:lsa} is to estimate $\ts$ based on the noisy versions $(H_t,g_t)$ of $(H,g)$.
There are various ways of quantifying the error $\eb_t \doteq \tb_t - \ts$ of estimating $\ts$ using $\tb_t$, but the magnitude
of the errors is usually measured in some weighted quadratic norm. The expected loss for time step $t$ then becomes
\begin{align*}
L_t \doteq \EE{ \norm{\eb_t}_{C}^2 }\,,
\end{align*}
where recall that for a SPD matrix $C$, $\norm{x}_C^2 = x^\top C x$.
Here, the choice of the matrix $C$ depends on the application.


As noted in the introduction, LSA is widely used in science and engineering. Here, we provide a specific example to be used throughout the rest of the paper.
\comment{
Here, we provide two specific examples
of LSA, which we will use throughout the paper:
%%%%%%%%%%%%%%%%%%%%%
\begin{example}[Linear Prediction under Quadratic Loss]
Fill this in similarly to the TD example. What is the data, what is the goal (here we can explain briefly),
what is the update, what is the loss metric.
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 
\end{example}
%%%%%%%%%%%%%%%%%%%%%
}

%%%%%%%%%%%%%%%%%%%%%
\begin{example}[Temporal Difference  (TD) Learning]
The simplest temporal differencelearning algorithm with a constant stepsize $\alpha>0$ is given by
\begin{align}
\label{tdzero}
\begin{split}
\theta_t
& = \theta_{t-1}+\alpha\, (R_t+ \gamma \ip{\theta_{t-1},\tilde{\phi}_t } - \ip{\theta_{t-1},\phi_t})\phi_t \\
& = \theta_{t-1}+\alpha\, \bigl( R_t\phi_t- (\phi_t \phi_t^\top-\gamma \phi_t \tilde{\phi}_t^\top)\theta_{t-1}\bigr)\,.
\end{split}
\end{align}
Here, $(\phi_t,\tilde{\phi}_t,R_t)\in \R^n\times \R^n \times \R$ is the data at time $t$.
Assuming $(\phi_t,\tilde{\phi}_t,R_t)$ is i.i.d. and say, bounded, 
we see that with $H_t = \phi_t \phi_t^\top-\gamma \phi_t \tilde{\phi}_t^\top$, $g_t = R_t \phi_t$ is in the form of 
\eqref{eq:lsa1} and satisfies the assumptions as long as $H = \EE{H_t}$ is PD. 
Note that the matrices $H_t$ are rank-1, hence the algorithm can indeed be implemented with $O(n)$ storage and
time (as is also shown in the first form of the update of $\theta_t$).
However, the matrices $H_t$ are not symmetric (nor is $H$ symmetric in general).
\todoc[inline]{Finish: This is indeed satisfied in ``on policy'' TD, etc. Add at least one reference. Note that this is TD(0).}
In TD learning the error of a parameter $\theta$ is commonly measured using the expected squared 
prediction error $L_t(\theta) = \EE{ (\phi_t^\top \theta - \phi_t^\top \ts)^2 } = \norm{\theta - \ts}_C^2$,
where $C = \EE{ \phi_t \phi_t^\top }$ is assumed to be PD.
\if0
\begin{align}\label{tdzero}
\theta_t=&\theta_{t-1}+\alpha \big(\phi(s_t) R(s_t)\nn\\&+ (\phi(s_t)\phi^\top(s_t)-\gamma \phi(s_t)\phi^\top(s'_t))\theta_{t-1}\big),
\end{align}
where are $s_t,s'_t\in S$ with $S$ being a finite set, $\phi(s)\in \R^n$ are the so called \emph{feature} vectors, $\gamma\in (0,1)$ is a given discount factor and $R\colon S\ra \R$ is map from S to reals. \todoc{Shoot, I wish we used $d$ in place of $n$..}
Here, $s_t$ are $i.i.d$ random variables distributed according to the law $s_t\sim \xi$ where $\xi$ is supported on $S$ and $s'_t\sim p(s_t,\cdot)$. The TD algorithm in \eqref{tdzero} can be cast in the general form as presented in \eqref{eq:lsa}, by letting $g_t=\phi(s_t)R(s_t)$ and $H_t=\phi(s_t)\phi^\top(s_t)-\gamma \phi(s_t)\phi^\top(s'_t)$.
\fi
\end{example}
%%%%%%%%%%%%%%%%%%%%%
The following theorem is the main result of the paper.
To state the result we define the key quantity
\begin{align}
\label{eq:rhodef}
\rho_\alpha\eqdef \inf_{\norm{x}=1} x^\top (H^\top+H-\alpha\EE{ H_1^\top H_1} )x\,.
\end{align}
\begin{theorem}\label{maintheorem}
Assume that $\alpha>0$ is such that $\rho_\alpha>0$. Let $\sigma^2 = \sigma_N^2 + (\ts)^\top \Sigma_M \ts$.
Then, it follows that for \emph{any} $t\ge 1$,
\begin{align}
\begin{split}
\MoveEqLeft\E[\norm{(\tb_t-\ts)}^2]\leq \\
&\left(1+4\frac1{\alpha\rho_{\alpha}}\right) \frac{1}{\alpha\rho_{\alpha}}
 \left(\frac{\norm{\theta_0-\ts}^2}{(t+1)^2}+\frac{\alpha^2\sigma^2}{t+1} \right)\,.
\end{split}
\end{align}
\end{theorem}
\todoc[inline]{The result should rather be stated for $\EE{\norm{\tb_t-\ts}_C^2}$.}
