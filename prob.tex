\section{Problem Setup}
We consider linear stochastic approximation (LSA) algorithms of the form
\begin{align}\label{lsa}
\theta_{t}=\theta_{t-1}+\alpha(g_t-H_t\theta_{t-1}),
\end{align}
where $\theta_t\in \R^n$, $\alpha>0$ is a positive step-size, $g_t\in \R^n$ and $H_t\in \R^{n\times n}$. The Rupert-Polyak average of the iterates $\theta_t$ in \eqref{lsa} is defined as
\begin{align}\label{rp} \tb_t\eqdef \frac{1}{t+1}\ous{\sum}{s=0}{t} \theta_s. \end{align}
We are interested in the behaviour of \eqref{lsa} under the following general conditions.
\begin{assumption}\label{genlsa}
\begin{enumerate}
\item\label{mart} $H_t\eqdef H+M_t$, $g_t\eqdef g+N_t$, where $M_t\in \R^{n\times n}$ and $N_t\in \R^n$ are martingale difference sequences with respect to a increasing sequence of $\sigma$-fields given by $\F_t\eqdef \{\theta_s, N_s,M_s,s\leq t\}$, i.e., $\E[N_t|F_{t-1}]=0,\E[M_t|F_{t-1}]=0,~\forall t\geq 0$.
\item \label{noise} The noise sequences satisfy $\E[N_t^\top N_t|F_{t-1}]\leq \sigma_1^2$ and $\E[M_t^\top M_t|F_{t-1}]\leq \Sigma_2^2$, where $\sigma_1^2>0$ is a positive scalar and $\Sigma^2$ is a real symmetric positive definite matrix.
\item \label{mat} $H$ is invertible and there exists $\ts=H^{-1}g\in\R^n$. Further, there exists a scalar $\sigma_2^2$ such that ${\ts}^\top\Sigma_2^2\ts\leq \sigma_2^2$.
\end{enumerate}
\end{assumption}
Here, $\ts$ is a desired fixed point or extremum, and the LSA in \eqref{lsa} computes $\ts$ from noisy data presented in the form of $g_t$ and $H_t$.\par
%The conditions in \Cref{genlsa} are very general in nature and

\textbf{Linear Stochastic Error Recursion (LSER)} The error dynamics for the LSA in \eqref{lsa} i.e., the dynamics of $e_t\eqdef\theta_t-\ts$ can be written as follows:
\begin{align}\label{errprelim}
\theta_t-\ts=\theta_{t-1}-\ts+\alpha_t(g_t -H_t(\theta_t-\ts+\ts))\nn\\
e_t=(I-\alpha_t H_t)e_{t-1}+(N_t-M_t\ts).
\end{align}
In what follows we consider what we call linear stochastic error recursion (LSER) given by
\begin{align}\label{lsergen}
e_t=(I-\alpha_t H_t)e_{t-1}+\zeta_t,
\end{align}
where $\zeta_t\in \R^n$ is a noise sequence that satisfies the following assumption
\begin{assumption}\label{zetaassmp}
$\zeta_t\in \R^{n}$ is a martingale difference sequence with respect to a increasing sequence of $\sigma$-fields given by $\F_t\eqdef \{e_s, \zeta_s,s\leq t\}$, i.e., $\E[\zeta_t|F_{t-1}]=0,~\forall t\geq 0$. Further, $\E[\zeta_t^\top \zeta_t|F_{t-1}]\leq \sigma^2$.
\end{assumption}
Note that \Cref{zetaassmp} holds for $\zeta_t=N_t-M_t\ts$, where $N_t$ and $M_t$ are as in \Cref{genlsa}, by choosing $\sigma^2=\sigma_1^2+\sigma_2^2$.\par

The behaviour of \eqref{lsa} is affected by the specific spectral/structural properties of $H$ and the structural properties of $\Sigma$.
In what follows, we quantify these specific properties.\\
\textbf{Different Cases of Matrix $H$}
\begin{enumerate}[label=(\roman*)]
\item The \emph{Asymptotically Stable} (AS) case: All the real parts of Eigen values of $H$ are positive.
\item The \emph{Positive Stable} (PS) case: $H$ is such that $x^\top H x>0, \forall x\in \R^n,x\neq 0$.
\item The \emph{symmetric positive definite stable} (SPDS) case: $H$ is real symmetric and $x^\top Hx>0, \forall x\in \R^n, x\neq 0$.
\end{enumerate}
We note that SPDS $\Ra$ PS $\Ra$ AS.\\
\textbf{Different Cases of Noise}
\begin{enumerate}[label=(\roman*)]
\item The \emph{Additive Noise} (AN) case: Here $M_t=0, ~\forall t\geq 0$.
\item The \emph{Multiplicative Noise} (MN) case: Corresponds to the general case wherein $M_t\neq 0$.
\item The \emph{Structured Noise} (SN) case: when $H$ is SPDS and $\E[\zeta_t^\top \zeta_t]\leq \sigma^2 H$, where $\zeta_t\eqdef N_t-M_t\theta^*$ and $\sigma>0$.
\end{enumerate}
\textbf{Performance Metrics} By defining $\eb_t=\frac{1}{t+1}\ous{\sum}{i=0}{t} e_i$, we have $\tb_t-\ts=\eb_t$. The finite time mean squared error is then given by $\E\norm{C(\tb_t-\ts)}^2=\E\norm{C\eb_t}^2$, where $C$ is any $n\times n$ matrix. The choices of $C$ we will be using are $C=I$ ($I$ is the identity matrix), $C=H$ and $C=H^{\frac{1}{2}}$.

\comment{
\textbf{Step-sizes}
\begin{enumerate}[label=(\roman*)]
\item \emph{Diminishing step-size} (DS) case: Here $\alpha_t\ra 0$ as $t\in \infty$ and $\sum \alpha_t^2<\infty$.
\item \emph{Constant step-size} with Rupert-Polyak  (CSRP) averaging case: Here $\alpha_t=\alpha>0,~\forall t\geq 0$. The Rupert-Polyak (RP) averaging defines the averaged iterates as
\begin{align}\label{rp} \tb_t\eqdef \frac{1}{t+1}\ous{\sum}{s=0}{t} \theta_s \end{align}
\end{enumerate}
}
\comment{\textbf{Performance Metrics}
\begin{enumerate}[label=(\roman*)]
\item The \emph{asymptotic} behaviour is quantified by looking at $\us{\lim}{t\ra \infty} \norm{\theta_t-\ts}$.
\item The \emph{finite time} behaviour is considered only for the CSRP case and is quantified by $\E\norm{C\tb_t-\ts}^2$, where $C$ is any $n\times n$ matrix. The choices of $C$ we will be using are $C=H$ and $C=H^{\frac{1}{2}}$ (only when in the SPDS-SN case).
\end{enumerate}
}
\section{Main Results}
\subsection{Error Dynamics}
Letting $F_{i,j}\eqdef (I-\alpha_i H_i)\ldots (I-\alpha_j H_j),~\forall i\geq j$ and $F_{i,j}\eqdef I,~\forall i<j$ we can recurse backwards in \eqref{errprelim} to obtain the following:
\begin{align}\label{errrec}
e_t=F_{t,1}e_0+\ous{\sum}{k=1}{t}\alpha F_{t,k+1}\zeta_k
\end{align}
Looking at \eqref{errrec} it is clear that quantities $F_{t,j}, j=1,\ldots,t$ influence the dyanmics of $e_t$.
%To be more specific, we can expect the following to dictate the dynamics of \eqref{errrec}.
\comment{
\begin{enumerate}
\item The spectral radius $\rho_s(F_{i,j})$, where $\rho(A)\eqdef\max\{|\lambda_1|,\ldots,|\lambda_k|,k\leq n\}$ with $|\lambda_i|$ denoting the absolute value of the Eigen value $\lambda_i$ of any given matrix $A$.
\item The expected operator norm of $\rho_{op}(F_{i,j})\E[\norm{F_{i,j}}_{op}],i\geq j$, where $\norm{A}_{op}\eqdef\us{\su}{x^\top x\leq 1}\sqrt{x^\top A^\top A x}$ is the operator norm of any given matrix $A$.
\end{enumerate}
}
