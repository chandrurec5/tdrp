\section{Problem Setup}
We study the finite time performance of LSERs in \eqref{lser} given as
\begin{align}\label{lser}
e_{t+1}=(I-\alpha_t H_t)e_t+ M_{t+1},
\end{align}
where
\begin{assumption}\label{fpassump}
\begin{enumerate}[leftmargin=*]
\item $\alpha_t=\alpha>0,~\forall t\geq 0$.
\item $H_t=H+N_{t+1}$ such that $\{N_t\}$ and $\{M_t\}$ are martingale difference sequences measurable with respect to an increasing family of $\sigma$-fields $\mathcal{F}_{t}\stackrel{\cdot}{=}\sigma(x_0,M_1,N_1\ldots,N_t,M_t),t\geq 0$, such that $\E[N_{t+1}|\F_t]=\E[M_{t+1}|\F_t]=0$, $\E[\parallel M_{t+1} \parallel^2|\F_t]\leq \sigma^2, \E[\parallel N_{t+1} \parallel^2|\F_t]\leq \sigma^2, ~\forall t\geq 0$ for some variance $\sigma>0$.
\end{enumerate}
\end{assumption}
\begin{align}\label{erp}
\bar{e}_{t}=\frac{1}{t}\overset{t-1}{\underset{s=0}{\sum}}e_t,
\end{align}
We are interested in the quantity
\begin{align}\label{errfun}
\err(t)&= \E[\parallel \eb_t\parallel^2]
%&=\B(t)+\V(t),
\end{align}
\subsection{Error Analysis - Bias, Variance and Product of Random Matrices}
We now expand $\err(t)$ to understand the structure of LSERs. For the purpose of our analysis, we define the product of random matrices $F_{j,i}\eqdef\Pi_{t=i}^{j} (I-\alpha H_t), \forall j\geq i$ and $F_{j,i}\eqdef I,~\forall i<j$.
\begin{align}
<\eb_t,\eb_t>=\frac{1}{t^2}\E <\overset{t-1}{\underset{i=0}{\sum}}e_i ,\overset{t-1}{\underset{i=0}{\sum}} e_i>,
\end{align}
Then we have from \eqref{lser} that for any $j>i$,
\begin{align}
e_{j+1}=F_{j,i}e_i + \sum_{k=i}^j F_{j,k+1}M_{k+1}
\end{align} and hence notice that
\begin{align}
\overset{t-1}{\underset{i=0}{\sum}}e_i=\overset{t-1}{\underset{i=0}{\sum}} F_{i,0}e_0+ \overset{t-1}{\underset{i=0}{\sum}} \overset{i}{\underset{k=0}{\sum}} F_{i,k+1}M_{k+1}\end{align} Thus the error can be naturally split into two terms namely $\err(t)=\B(t)+\V(t)$, where $\B(t)$ is the \emph{bias} term which satisfies $\err(t)=\B(t)$ when $M_{t+1}=0~\forall t\geq 0$ and $e_0\neq 0$,  and, $\V(t)$ is the variance term which satisfies $\err(t)=\V(t)$ when $e_0=0$ and $M_{t+1}\neq 0$.
%Now, for any $j>i$ we have $<e_i,e_j>=<e_i,F_{j,i}e_i+\sum_{k=i}^j F_{j,k+1}M_{k+1}>$
\begin{lemma}
For any $x_i\in \R^n$ that is $\F_i$ measurable and $\forall ~j \geq k> i$ it follows that $\E[x_i^\top F_{j,k+1}M_{k+1}]=0$
\end{lemma}
\begin{lemma}
$\E <e_i,F_{j,i}e_i>=(I-\alpha H)^{j-i}e_i$
\end{lemma}
\begin{remark}
$<e_j,e_j>=<F_{j,i}e_i+\sum_{k=i}^j F_{j,k+1}M_{k+1},F_{j,i}e_i+\sum_{k=i}^j F_{j,k+1}M_{k+1}>$, which involves expectation of product of random matrices of the form
$
\E[(I-\alpha H_i)^\top\ldots (I-\alpha H_j)^\top (I-\alpha H_j)\ldots(I-\alpha H_i)]
$.
\end{remark}
\begin{assumption}
Let $\rho_{s}(\alpha)\eqdef \underset{{x\in \R^n}}{\sup}x^\top (2H-\alpha E[H_t^\top H_t])x $ and $\rho_d\eqdef\parallel H \parallel$ (where $\parallel \cdot\parallel$ is the operator norm of the matrix). Then we assume that exist an $\alpha_{\max}$ such that $~\forall \alpha\in (0,\alpha_{\max})$
\begin{enumerate}[leftmargin=*]
\item $2H- \alpha E[H_t^\top H_t] >0$.
\item $|1-\rho_s(\alpha)|<1$.
\item $|1-\alpha\rho_d|<1$.
\end{enumerate}
\end{assumption}

\begin{lemma}\label{nonsym}
For $\alpha\in(0,\alpha_{\max})$ and $H$ non-symmetric
\begin{align}
\err(t)\leq &\frac{(\rho_s\alpha)^{-1}(1+2(\rho_d\alpha)^{-1})}{t^2}\nn\\&+\frac{\alpha^2(\alpha\rho_s)^{-1}(1+2(\rho_d\alpha)^{-1})}{t}
\end{align}
\end{lemma}
\begin{lemma}\label{sym}
For $\alpha\in(0,\alpha_{\max})$ and $H$ symmetric
\begin{align}
\err(t)\leq &\frac{(\rho_s\alpha)^{-1}(2(\rho_d\alpha)^{-1})}{t^2}\nn\\&+\frac{\alpha^2(\alpha\rho_s)^{-1}(2(\rho_d\alpha)^{-1})}{t}
\end{align}
\end{lemma}
\section{LSAs to solve linear system of equations}
In what follows, we assume that we have access to the stochastic orcales which provide $i.i.d$ samples of $A$ and $b$. We now look at the three different LSAs to solve \eqref{linsys}.
\subsection{Fixed Point Linear Stochastic Approximation (FP-LSA)}
We call the following stochastic recursion to compute $\theta^*$ a FP-LSA algorithm
\begin{align}\label{fplsa}
\theta_{t+1}=\theta_t+\alpha(b_t-A_t),
\end{align}
where we assume that
\begin{assumption}
\begin{enumerate}[leftmargin=*]
\item $b_t$ and $A_t$ are $i.i.d$ samples of $b$ and $A$
\item \label{pd}$\underset{x\in\R^n}{\inf} x^\top A x>0$.
\end{enumerate}
\end{assumption}
Notice that
We now present the LSER corresponding the FP-LSA in \eqref{fplsa}.
\begin{align}\label{fplser}
e_{t+1}=(I-\alpha A_t)e_t+\alpha(M^{(2)}_{t+1}-M^{(1)}_{t+1}\ts),
\end{align}
where $M^{(1)}_{t+1}=A_t-A$, $M^{(2)_{t+1}}=b_t-b$,  and $\ts=A^{-1}b$. Now picking $\sigma=\sigma_1^2+\sigma^2_2 \parallel\ts\parallel^2$ it follows that $\E[\parallel M_{t+1} \parallel^2|\F_t]<\sigma^2$ results of \Cref{nonsym} applies to the LSER in \eqref{fplser}.
\comment{The error term splits into $\err(t)= \B(t)+\V(t)$, (we have dropped $\Sigma$ for the sake of brevity) where $\B(t)$ is the bias term which satisfies $\err(t)=\B(t)$ when $M^{(i)}_{t+1}=0,i=1,2,~\forall t\geq 0$ and $x_0\neq x^*$ and $\V(t)$ is the variance term which satisfies $\err(t)=\V(t)$ when $x_0=x^*$ and $M^{(i)}_{t+1}\neq 0$.}
\subsection{Least-Squares Linear Stochastic Approximation (LS-LSA)}
In general \Cref{pd} need not hold and we would still wish to solve \eqref{linsys}. One way to ensure \Cref{pd} is to ensure that the system is positive definite which can be achieved by solving for $A^TA\theta^*=A^\top b$. We call the following stochastic recursion to compute $\theta^*$ a LS-LSA algorithm
\begin{align}\label{lslsa}
\theta_{t+1}=\theta_t+A^{(1)\top}_t(b_t-A^{(2)}_t\theta_t),
\end{align}
where we assume
\begin{assumption}\label{lsassump}
$b_t$, $A^{(1)}_t$and $A^{(2)}_t$ are $i.i.d$ samples of $b$ and $A$.
\end{assumption}
The LSA in \eqref{lslsa} is a stochastic gradient since the $\nabla_\theta \parallel b-A\theta\parallel^2=A^\top(b-A\theta)$. The LSER corresponding to LS-LSA in \eqref{lslsa} is given by
\begin{align}
e_{t+1}=&(I-\alpha A^{(1)\top}_tA^{(2)}_t)e_t+\alpha(A^\top M^{(3)}_{t+1}+M^{(1)\top}_{t+1}M^{(3)}_{t+1}\nn\\&-A^\top M^{(2)}_{t+1}\ts-M^{(1)\top}_{t+1}M^{(2)}_{t+1}\ts),
\end{align}
where $M^{(1)}_{t+1}=A^{(1)}_t-A$, $M^{(2)}_{t+1}=A^{(2)}_t-A$ and $M^{(3)}_{t+1}=b_t-b$. Choosing $\sigma=\parallel A\parallel^2 (\sigma_2^2+\sigma_1^2\parallel \ts\parallel^2)+ \sigma_1^4(1+\parallel \ts\parallel^2)$ the results of \Cref{sym} applies to the LSER in \eqref{lslser}.

\subsection{Saddle-Point Linear Stochastic Approximation (SP-LSA)}
An important downside of \Cref{lslsa} is that two independent samples of $A$ are available in the form of $A^{(1)\top}_t$ and $A^{(2)}_t$ which might be possible in certain applications. This issue can be alleviated by looking at the following Saddle-Point objective function
\begin{align}
L(\theta,y)=\min_{\theta\in \R^n}\max{y\in \R^n}<b-A\theta,y>-\frac{1}{2}\parallel y\parallel ^2
\end{align}
By noticing the fact that $\nabla_\theta L =-A^\top y$ and $\nabla_{y}L=-A\theta-I$, we now specify the SP-LSA algorithm as follows:
\begin{align}\label{splsa}
\begin{split}
y_{t+1}&=y_t+\alpha (b_t-A_t\theta_t-y_t)\\
\theta_{t+1}&=\theta_t+\alpha(A_t^\top)y_t
\end{split}
\end{align}
The SLER corresponding to the SP-LSA in \eqref{splsa} is given by
\begin{align}
e_{t+1}=\Big(\begin{bmatrix} I & 0 \\ 0 &I\\\end{bmatrix} -\begin{bmatrix} I & A_t \\ -A_t^\top &0\\\end{bmatrix}\Big)e_t+\Big(\begin{bmatrix} M^{(2)}_{t_1}-M^{(1)}_{t+1}\ts  \\ 0\\\end{bmatrix}\Big),
\end{align}
where $M^{(1)}_{t+1}=A_t-A$, $M^{(2)_{t+1}}=b_t-b$,  and $\ts=A^{-1}b$.
Let $x_t=(y_t,\theta_t)$, $g_t=\begin{bmatrix} b_t\\ 0\\\end{bmatrix}$ and $G^{(i)}_t=\begin{bmatrix} I & A^{(i)}_t \\ -A^{(i)\top}_t &0\\\end{bmatrix}$ for $i=1,2$, then consider the following updates
\begin{align}
x'_{t}&=x_t+\alpha (g_t-G^{(1)}_t x_t)\\
x_{t+1}&=x_t+\alpha(g_t- G^{(2)}_t x'_t);
\end{align}
\comment{
The problem of solving a linear system of equations arises in many machine learning applications. Formally, the aim is to solve the following linear system given by
\begin{align}\label{linsys}
Hx^*=g,
\end{align}
where $H\in \R^{n\times n}$ and $g\in \R^{n\times n}$ are respectively the given design matrix and design vector which are dependent on the underlying data. In many applications of interest \cite{fbach,rl}, the solution $x^*$ of \eqref{linsys} has to be computed using \emph{noisy} measurements of $g$ and $H$. In such applications, a common approach to handle noisy data is to employ a stochastic approximation (SA) algorithm, which computes $x^*$ by updating its the iterates in an incremental manner. In this paper, we focus on the linear stochastic approximation (LSA) algorithm to solve \eqref{linsys}, given by the following recursion:
\begin{align}\label{linearrec}
x_{t+1}=x_t+\alpha_t (g_t-H_t x_t)
\end{align}
where $x_t\in \R^n$ are iterates, $\alpha_t>0,t\geq 0$ are  step-sizes. Further, in \eqref{linearrec} $g_t\in \R^n$ and $H_t\in \R^{n\times n}$ are \emph{noisy} measurements of $g$ and $H$, i.e., $g_t\eqdef g+M^{(1)}_{t+1}$ and $H_t\eqdef H+M^{(2)}_{t+1}$ for some noise sequences $\{M^{(i)}_{t},i=1,2,~ t\geq 0\}$.
Note that $\rho_s(\alpha)>1$ is the case of SP-LSA. To see this,

\subsection{Objectives and Assumptions}
We are interested in the behaviour of the iterates $\{x_t, t\geq 0\}$ of the LSA \eqref{linearrec}, and in particular the rate of convergence to the solution $x^*$.
It is natural to expect that the behavior of the iterates varies with the design variables ($H$ in particular) and the properties of the noise sequence, and it is desirable to build algorithms that are \emph{robust} to these variations. The step-sizes are completely decided by the algorithm designer and hence their choice is critical in ensuring robustness.
}
